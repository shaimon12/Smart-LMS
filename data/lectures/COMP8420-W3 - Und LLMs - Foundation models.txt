This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or has licence to 
use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Lecture 3 | Date: 14th March 2025 | Prof. Longbing Cao | 14SCO 100 Theatrette
COMP 8420 Advanced NLP
UNDERSTANDING LLMS - FOUNDATION MODELS
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Staff Profile
Prof Longbing Cao
Distinguished Chair in AI
ARC Future Fellow (professor)
Director, Centre for Frontier AI Research
E: longbing.cao@mq.edu.au
W: www.DataSciences.Org 
OFFICE | FACULTY | DEPARTMENT
2
Office hours: Fri 10am-5pm (S1)
Location: 321, 4 Research Park Drive
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Unit: 
• Lectures for Weeks 3, 4, 5, 6, 7, 12
• Workshops for these weeks by Weijun Li
• Aims and approaches
• A body of knowledge of large language models (and generative 
AI)
• From foundations to practices
• LLMs/NLP advances
• Industry/mainstream models, skills and tools
Agenda - Unit
OFFICE | FACULTY | DEPARTMENT
3
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Unit introduction - LLMs
OFFICE | FACULTY | DEPARTMENT
4
Week
Lectures
Workshops
3
Transformers
BERT-base, RoBERTa-base 
and GPT-2
4
Text classification, sentiment analysis (Assignment 1)
BERT-base, T5-base
5
MLLMs (Assignment 2)
openai/clip-vit-base-
patch32
6
LLMs training and fine-tuning
Prompt engineering, reasoning (Assignment 2)
meta-llama/Llama-3.2-1B
LoRA Adaptation
7
Comparing LLMs/MLLMs
GPT-4o-mini, Llama
12
Assignments
Assignment 3 QA
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Large language models (LLMs) 
• Foundations - revisit
• Tokenization
• Embeddings  
• Transformers
• Overview 
• Layered architectures 
• Self-attention and multihead attention
• Embeddings 
• Transformers for Classification
• Transformer opportunities 
• Workshops: Transformer models
Agenda – This week
OFFICE | FACULTY | DEPARTMENT
5
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Textbooks
• Natural Language Processing with Transformers, Revised Edition by Lewis Tunstall, Leandro von 
Werra, Thomas Wolf. Publisher: O’Reilly Media, Inc.
     https://multisearch.mq.edu.au/permalink/61MACQUARIE_INST/1c87tk9/alma9963496110802171
• Hands-on Large Language Models by Jay Alammar, Maarten Grootendorst, Publisher: O’Reilly 
Media, Inc.
     https://learning.oreilly.com/library/view/hands-on-large-language/9781098150952/
• References
• Hands-On Machine Learning with Scikit-Learn and TensorFlow, by Aurélien Géron (O’Reilly)
• Deep Learning for Coders with fastai and PyTorch, by Jeremy Howard and Sylvain Gugger (O’Reilly)
• Natural Language Processing with PyTorch, by Delip Rao and Brian McMahan (O’Reilly)
• The Hugging Face Course, by the open source team at Hugging Face
• Online resources/public domains
• The slides may involve materials from online, public domains, and other lectures etc., do not share 
the slides
References for the unit
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this 
material publicly online without permission. Macquarie University is the copyright owner of (or has licence to use) the intellectual property in 
this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Foundation models: 
Overview of LLMs
DEEP LEARNING FOR NLP: FROM 
TRANSFORMERS TO LLM, GENAI
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
NLP taxonomy
OFFICE | FACULTY | DEPARTMENT
8
• NLP: 
• Linguistic to 
representation-driven 
• Text processing to 
representation 
learning and 
generation
• Unimodal to 
multimodal
• Language model to 
vision-language-x 
modelling
• LLMs to MLLMs
Exploring the Landscape of Natural Language Processing 
Research
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
NLP taxonomy
OFFICE | FACULTY | DEPARTMENT
9
• NLP: 
• Linguistic to 
representation-driven 
• Text processing to 
representation 
learning and 
generation
• Unimodal to 
multimodal
• Language model to 
vision-language-x 
modelling
• LLMs to MLLMs
Exploring the Landscape of Natural Language Processing 
Research
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
LLMs concept map
OFFICE | FACULTY | DEPARTMENT
10
Adv NLP
Understanding
LLMs
Foundations
Tokenization
Embedding
Transformers
Landscape &
Futures
LLM families
LLM models
Risk & 
mitigation
Interpretation
Alignment
Using LLMs
Text 
classification
Text generation
Text 
summarization
Text sentiment
Developing 
LLMs
Multimodal 
LLMs
MLLM overview
MLLM 
architectures
MLLM tasks
MLLM 
applications
MLLM for 
humanoids
Training & fine-
tuning
Prompt 
engineering
Chain of 
thought
Reasoning 
RAG
RLHF
Distillation
LLM
• LLMs  MLLMs
• Understanding LLMs
• Foundations
• Landscapes 
• Futures
• Developing LLMs
• Training & fine-tuning
• Multimodal LLMs
• Using LLMs
• Text classification/sentiment
• Text generation
• Text summarization
• Practices - Workshops
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Foundation models
• Original Transformer 
model
• Encoder-based models
• Decoder-based models
• Hybrid models
• Modality-oriented models: 
• Vision transformer
Foundations: Transformers
OFFICE | FACULTY | DEPARTMENT
11
https://learning.oreilly.com/library/view/natural-language-processing
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this 
material publicly online without permission. Macquarie University is the copyright owner of (or has licence to use) the intellectual property in 
this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Tokenization - Revisit
TOKENS
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Tokenization
OFFICE | FACULTY | DEPARTMENT
13
• Tokens: Individual words or 
subwords
•
Word tokens, subword 
tokens, character tokens, byte 
tokens
• Tokenization: Convert 
sentences to tokens, eg by 
whitespaces
• Vocabulary: A bag of words at 
a vocabulary size
• Bag of words (BoW) model: 
•
Create representations of text 
by vectors/vector 
representations (numbers)
•
Document embeddings
• One-hot representation
• Representation models
https://learning.oreilly.com/library/view/hands-on-large-language/
from transformers import AutoModelForCausalLM, AutoTokenizer 
# Load model and tokenizer 
model = AutoModelForCausalLM.from_pretrained( 
    "microsoft/Phi-3-mini-4k-instruct", 
    device_map="cuda", 
    torch_dtype="auto", 
    trust_remote_code=True, 
) 
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this 
material publicly online without permission. Macquarie University is the copyright owner of (or has licence to use) the intellectual property in 
this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Embedding - Revisit
EMBEDDINGS
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Embedding
OFFICE | FACULTY | DEPARTMENT
15
• Embedding
• Vector representations of data that 
attempt to capture its meaning
• Representation models/learning
• Vector embeddings
• Semantic representation: Vector 
representations of semantic/meaning 
of words/text
• word2vec for word embedding
• Word embeddings
• Many properties to represent a word
• Semantic similarity between words
• Feed into LLMs/language models 
https://learning.oreilly.com/library/view/hands-on-large-language/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Embedding
OFFICE | FACULTY | DEPARTMENT
16
• Types of embeddings
• Token embedding
• Word embedding: word2vec
• Sentence embedding
• Document embedding: BoW
• Context embedding
• The context in which a word/token 
appears by ‘attending’ to the entire 
sentence
• Attention: 
• Weight/relevance of contextual 
words/tokens on the target 
• Parallelization of training
• Encoding
• Decoding
https://learning.oreilly.com/library/view/hands-on-large-language/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this 
material publicly online without permission. Macquarie University is the copyright owner of (or has licence to use) the intellectual property in 
this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformers
FOUNDATION MODEL OF 
MODERN NLP AND LANGUAGE 
MODELING
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer - References
OFFICE | FACULTY | DEPARTMENT
18
• Read the Attention Is All You Need paper: https://arxiv.org/abs/1706.03762
• Watch Łukasz Kaiser’s talk walking through the model and its details
• Play with the Jupyter Notebook provided as part of the Tensor2Tensor repo
• Blogs:
• https://jalammar.github.io/illustrated-transformer/
• https://jinglescode.github.io/2020/05/27/illustrated-guide-transformer/ 
• https://towardsdatascience.com/transformers-141e32e69591
• https://medium.com/analytics-vidhya/neural-machine-translation-using-bahdanau-attentionmechanism-
d496c9be30c3
• Video: https://www.youtube.com/watch?v=TQQlZhbC5ps
• Roberta: https://arxiv.org/abs/1907.11692
• Swin Transformer: Hierarchical Vision Transformer using Shifted Windows: https://arxiv.org/abs/2103.14030
• Attention Heads of Large Language Models: A Survey: https://arxiv.org/abs/2409.03752
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer - Architecture
OFFICE | FACULTY | DEPARTMENT
19
• Transformers
Encoder (e.g., BERT)
Decoder (e.g., GPT)
https://arxiv.org/abs/1706.03762
Attention is All You Need
I am a student
Sono uno studente
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• fromscratchtransformer.py 
Transformer - Code
OFFICE | FACULTY | DEPARTMENT
20
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Self-attention
• Key, Query, Value 
• Create three vectors 
from embeddings
• Select a value 
(referenced by a key) 
relevant to a query 
(what trying to pull from 
input)
• Calculate a score for 
how much to focus on 
each part of the input 
when we encode words 
at specific positions
Transformer – Self-Attention
OFFICE | FACULTY | DEPARTMENT
21
https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-
analogy-for-understanding-transformers
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Calculate attention scores
• Score = qp * kp
• Multiply each value vector by the 
softmax score, keep intact the 
values of the word(s) we want to 
focus on, and drown-out irrelevant 
words (by multiplying them by tiny 
numbers like 0.001, for example)
• Sum up the weighted value vectors: 
This produces the output of the self-
attention layer at this position (for 
the first word)
• Resulting vector is one we can send 
along to the feed-forward neural 
network
Transformer - Self-Attention
OFFICE | FACULTY | DEPARTMENT
22
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Matrix calculation of 
attention scores
• Score = qp * kp
• Calculate the Query, Key, and 
Value matrices
• Pack our embeddings into a 
matrix X
• Multiply it by the weight 
matrices we’ve trained 
(WQ, WK, WV)
Transformer - Self-Attention
OFFICE | FACULTY | DEPARTMENT
23
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Output of self attention
• 𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝑄𝑄, 𝐾𝐾, 𝑉𝑉=
𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠
𝑄𝑄𝐾𝐾𝑇𝑇
𝑑𝑑𝑘𝑘𝑉𝑉
• Self-attention in a matrix 
form
Transformer – Self-Attention
OFFICE | FACULTY | DEPARTMENT
24
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
•Attention: 
• In processing each word (each 
position in the input sequence), 
self attention allows the model to 
look at other positions in the input 
sequence for clues of a better 
encoding for this word
• Attention focuses ‘it’ on ‘The 
animal’
Transformer – Self-Attention
OFFICE | FACULTY | DEPARTMENT
25
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Purposes
• Expand the model’s ability to focus on 
different positions: 
• in self-attention, each score (z1) 
contains a little bit of every other 
vectors but dominated by the actual 
word itself
• Give the attention layer multiple 
“representation subspaces”: 
• 8 sets of Query/Key/Value weight 
matrices, use eight attention heads
• Each of these sets is randomly initialized 
• After training, each set is used to project 
the input embeddings (or vectors from 
lower encoders/decoders) into a different 
representation subspace
Transformer - MultiHeadAttention
OFFICE | FACULTY | DEPARTMENT
26
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Multihead self-
attention
Transformer - MultiHeadAttention
OFFICE | FACULTY | DEPARTMENT
27
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Stack independent attention layers
• Concatenate outputs of attention 
heads
• Different positions
• Multiple representation subspaces
• Do not oversaturate one attention 
mechanism 
Transformer - MultiHeadAttention
OFFICE | FACULTY | DEPARTMENT
28
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
•In encoding the word "it“
• One attention head focuses 
most on "the animal" 
• Another focuses on "tired“
• But adding all head 
attentions make it harder to 
interpret
• Copes with order of 
sentence sequence
Transformer - MultiHeadAttention
OFFICE | FACULTY | DEPARTMENT
29
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Word embeddings
• Only encode words
• Missing the order of 
sentence sequence
• Positional embeddings
• Add a vector to each 
input embedding
• Represent the position 
of each word/token, or 
the distance between 
different words in the 
sequence
Transformer – Positional encoding
OFFICE | FACULTY | DEPARTMENT
30
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• fromscratchtransformer.py 
Transformer - Code
OFFICE | FACULTY | DEPARTMENT
31
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Rectified linear unit (ReLU)
• GELU(x) = x · Φ(x)
• Φ(x) is the cumulative distribution function of the Gaussian distribution
• Φ(𝑥𝑥) =
1
2 [1 + erf(
𝑥𝑥
2)]
• Gaussian error linear unit (GeLU)
• Empirically seems to work better than ReLU
• Gradient is quite smooth (so it doesn’t vanish abruptly)
• Better for generalising
Transformer - GeLU
OFFICE | FACULTY | DEPARTMENT
32
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• fromscratchtransformer.py 
Transformer - Code
OFFICE | FACULTY | DEPARTMENT
33
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• TransformerEncoderLayer
• LayerNorm
• Add & Normalize
• For both encoder and 
decoder
Transformer - 
TransformerEncoderLayer
OFFICE | FACULTY | DEPARTMENT
34
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Standardize (mean=0, stddev=1) inputs 
across features for each sample.
• Improve stability and accelerate 
convergence 
• Include learnable parameters: γ (scale), β 
(shift)
• 𝐿𝐿𝐿𝐿𝑥𝑥= γ
𝑥𝑥−μ
σ
+ β
• μ, σ: Mean, Std Dev (per feature, per sample)
• Essential for models sensitive to input scale 
(e.g., Transformers)
Transformer - LayerNorm
OFFICE | FACULTY | DEPARTMENT
35
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• fromscratchtransformer.py 
Transformer - Embeddings
OFFICE | FACULTY | DEPARTMENT
36
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• fromscratchtransformer.py 
Transformer – Embedding forward 
layer
OFFICE | FACULTY | DEPARTMENT
37
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• fromscratchtransformer.py 
Transformer – TransformerEncoder
OFFICE | FACULTY | DEPARTMENT
38
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer – Encoder
OFFICE | FACULTY | DEPARTMENT
39
• Encoder starts by processing the 
input sequence
• The output of the top encoder is then 
transformed into a set of attention 
vectors K and V
• These are to be used by each 
decoder in its “encoder-decoder 
attention” layer which helps the 
decoder focus on appropriate places 
in the input sequence
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer – Decoder
OFFICE | FACULTY | DEPARTMENT
40
• Decoding starts after finishing the 
encoding phase
• Each step in the decoding phase 
outputs an element from the output 
sequence 
• The output of each step is fed to the 
bottom decoder in the next time step, 
and the decoders bubble up their 
decoding results just like the encoders 
did
• Just like the encoder inputs, embed 
and add positional encoding to those 
decoder inputs to indicate the position 
of each word
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer – Final linear layer
OFFICE | FACULTY | DEPARTMENT
41
• Turn the vector of floats from the 
decoder stack into a word
• A simple fully connected neural 
network 
• Project the vector produced by 
the stack of decoders into a 
much, much larger vector called 
a logits vector
• Output: 
• A vector of logits – each cell 
corresponds to the score of a 
unique word
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer – Softmax layer
OFFICE | FACULTY | DEPARTMENT
42
• Turn those scores into probabilities 
(all positive, all add up to 1.0)
• The cell with the highest probability is 
chosen, and the word associated with 
it is produced as the output for this 
time step
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer – Architecture
OFFICE | FACULTY | DEPARTMENT
43
• Revisit the architecture
https://www.scribd.com/document/763444576/Attention-Is-All-You-Need-Presentation
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this 
material publicly online without permission. Macquarie University is the copyright owner of (or has licence to use) the intellectual property in 
this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer - Classification
TRAINING AND CLASSIFICATION
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer – Classification
OFFICE | FACULTY | DEPARTMENT
45
• fromscratchtransformer.py
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Train on labelled data
• Compare with the actual 
correct output
• Define output vocabulary in 
preprocessing phase before 
training 
Transformer – Training
OFFICE | FACULTY | DEPARTMENT
46
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Training
• Vector dimensionality: size of 
output vocabulary
• Word embedding: one-hot 
encoding
Transformer – Training
OFFICE | FACULTY | DEPARTMENT
47
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Classification
• Probability distribution of output words, 
sum to 1
• Loss function: 
• Probability of untrained word – 
correct desired output
• Each probability distribution is 
represented by a vector of width 
vocab_size (30k-50k words)
• Backpropagation to make the output 
closer to the desired output
• Optimize the model parameters 
initialized randomly
Transformer – Classification
OFFICE | FACULTY | DEPARTMENT
48
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Loss function
• Cross entropy:
• Kullback-Leibler divergence 
• 1st position has the highest 
probability at cell with word ‘I’
• Iterate all positions and their 
words
• Until for ‘eos’ (end of sentence) 
symbol – an element vocabulary
Transformer – Classification
OFFICE | FACULTY | DEPARTMENT
49
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
• Train on large dataset
• Generate the probability distribution 
of all words until eos
• Cross validation with multiple 
sample sets of the data
Transformer – Classification
OFFICE | FACULTY | DEPARTMENT
50
https://jalammar.github.io/illustrated-transformer/
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this 
material publicly online without permission. Macquarie University is the copyright owner of (or has licence to use) the intellectual property in 
this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer – Opportunities
TRAINING & APPLICATIONS
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer – Training
OFFICE | FACULTY | DEPARTMENT
52
• Semi-supervised learning
• Unsupervised pretraining over a very large dataset of general 
text
• Followed by supervised fine-tuning over a focused data set of 
inputs and outputs for a particular task
• Pretrained transformers
• Google’s BERT
• Hugging Face’s transformers
• GPT-3, etc.
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformers – Applications
OFFICE | FACULTY | DEPARTMENT
53
• Language modeling
• Text generation
• Reading comprehension
• Sentiment analysis
• Paraphrasing
• Neural translation
• Summarization
• Text classification
• Token classification
• Sentence similarity
• Semantic segmentation
• …
• Question answering
• Speech to text
• Vision transformers
• Sequence transduction
• Image classification
• Computer vision
• Object detection
• Time series forecasting 
• Speech recognition
• Behavior modeling
• …
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this 
material publicly online without permission. Macquarie University is the copyright owner of (or has licence to use) the intellectual property in 
this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Take-home Messages
INSIGHTS AND ARCHITECTURE
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Transformer – Foundation model
OFFICE | FACULTY | DEPARTMENT
55
•Transformer is a foundation model
•Families of Transformers 
•Customize your transformers 
•Trimming nodes
•Replacing floating point values by integer
•Training a model on the outputs of another model
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this material publicly online without permission. Macquarie University is the copyright owner of (or 
has licence to use) the intellectual property in this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Next week – Week 4 
OFFICE | FACULTY | DEPARTMENT
56
•Using LLMs for text processing: 
•LLMs for text classification, sentiment, 
generation and summarization
This material is provided to you as a Macquarie University student for your individual research and study purposes only. You cannot share this 
material publicly online without permission. Macquarie University is the copyright owner of (or has licence to use) the intellectual property in 
this material. Legal and/or disciplinary actions may be taken if this material is shared without the University’s written permission.
Question & Answer