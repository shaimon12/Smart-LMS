# COMP8420 Advanced Natural Language Processing
## Week 8 - LLM Arena

## Today's Topics  
In this session, we will explore and compare the capabilities of advanced LLMs through two board games:  
- Tic Tac Toe with LLMs
- Werewolf (LLM Edition)


Before the rise of LLMs, board games were a classic way to evaluate AI systems' capabilities.

Games like chess, Go, and Othello require strategic thinking, planning, and understanding of complex rules ‚Äî all traits that researchers aimed to develop in AI agents.

Milestones such as AlphaGo's victory over top Go players showed how AI could master difficult intellectual challenges.

<center>
    <img src="https://media.newyorker.com/photos/590975168b51cf59fc422f47/master/w_2240,c_limit/House-Alpha-Go-2.jpg" width="30%">
</center>

Today, we continue this tradition by putting advanced language models like Gemini, GPT, and Claude into an arena-style competition, using classic board games to test and compare their abilities.

üí° **NOTE**: Some models, such as GPT and Claude, are not free to access individually, but testing them is optional ‚Äî we will explore them together during class.

Code in this workshop is developed with Claude (Anthropic) and GPT (OpenAI).

#### 1. Tic Tac Toe with LLMs

The first game we will explore is Tic Tac Toe ([want to try it?](https://g.co/kgs/6F6RZf9)), where two players take turns marking spaces on a 3x3 grid, aiming to align three marks in a row.

Do you think LLMs can understand the rules and play a good game? Let's have a try!

<center>
    <img src="https://upload.wikimedia.org/wikipedia/commons/7/7d/Tic-tac-toe-animated.gif?20190126164540" width="20%">
</center>

We'll start with a simple setup: chatting with the Gemini model, letting it display the board, decide its own moves, and track the game state.

First, we define a helper function to handle communication with Gemini:

"""Chat with Gemini API and display responses neatly."""

import requests
import json
from IPython.display import display, Markdown

# API key configuration
GEMINI_API_KEY = "YOUR_GEMINI_API_KEY"  # Replace with your actual API key

def chat_with_gemini(messages, model="gemini-1.5-flash"):
    """Simple function to chat with a Gemini model and maintain conversation history."""

    # Configure API endpoint
    endpoint = f"https://generativelanguage.googleapis.com/v1/models/{model}:generateContent"
    params = {"key": GEMINI_API_KEY}
    headers = {"Content-Type": "application/json"}

    # Format the conversation history for Gemini API
    formatted_messages = [
        {"role": msg["role"], "parts": [{"text": msg["content"]}]}
        for msg in messages
    ]

    data = {
        "contents": formatted_messages
    }

    # Make API call
    try:
        response = requests.post(
            endpoint,
            headers=headers,
            params=params,
            json=data,
            timeout=10
        )

        if response.status_code != 200:
            print(f"Error with API ({response.status_code}): {response.text}")
            return "Sorry, I encountered an error. Let's try again."

        response_data = response.json()
        response_text = response_data["candidates"][0]["content"]["parts"][0]["text"].strip()

        # Add response to conversation history
        messages.append({"role": "model", "content": response_text})

        # Improved display that handles code blocks better
        display_text = f"**Gemini ({model})**:\n\n{response_text}"
        display(Markdown(display_text))

        return response_text

    except Exception as e:
        print(f"Error with API request: {str(e)}")
        return "Sorry, there was an error communicating with the API."

Next, we set up and manage the Tic Tac Toe game by guiding Gemini through an initial prompt and an interactive loop:

"""Interactive Tic Tac Toe game with Gemini"""

def start_tic_tac_toe_game(model="gemini-1.5-flash"):
    """Start a Tic Tac Toe game conversation with Gemini."""

    # Initialize conversation history
    messages = []

    # Initial game setup message with improved board display instructions
    initial_prompt = """Let's play Tic Tac Toe. You'll be 'O' and I'll be 'X'.

Please display the empty 3x3 board at the start, and then after each move.
Include coordinates or position references to make it easier for me to specify my moves.

After each of my moves, you should:
1. Update the board with my move
2. Check if I've won
3. If not, make your move ('O')
4. Update the board again
5. Check if you've won or if it's a draw

If anyone wins or the game is a draw, explicitly include one of these exact phrases in your response:
- "You win! Congratulations!"
- "I win! Good game!"
- "It's a draw!"

Format the board in whatever way you think is most clear and easy to read.
Let's start!"""

    # Add initial message to history
    messages.append({"role": "user", "content": initial_prompt})

    # Get first response from Gemini (should be the empty board)
    response = chat_with_gemini(messages, model)

    # Main game loop
    game_finished = False
    while not game_finished:
        # Get user move
        user_move = input("\nEnter your move: ")

        if user_move.lower() == 'quit':
            print("Game ended by player. Thanks for playing!")
            break

        # Add user move to conversation
        messages.append({"role": "user", "content": user_move})

        # Get model response
        response = chat_with_gemini(messages, model)

        # Check if game is over
        if "You win! Congratulations!" in response:
            print("\nGame over! You've won!")
            game_finished = True
        elif "I win! Good game!" in response:
            print("\nGame over! The AI has won!")
            game_finished = True
        elif "It's a draw!" in response:
            print("\nGame over! It's a draw!")
            game_finished = True

    print("Thanks for playing!")

Finally, we can start the game by calling:

## Uncomment the line below to play the game.
## You can change the model name to try different models, such as 'gemini-1.5-flash', 'gemini-2.0-flash', etc.
# start_tic_tac_toe_game(model="gemini-1.5-flash")

Here is one sample game session played with Gemini:

start_tic_tac_toe_game(model="gemini-1.5-flash")

üí° **Think about it:**
- Do you notice any issues in the game log?  
  - **Issue 1: Misplaced piece (hallucination in execution)**  
    Gemini said it placed at square 9, but actually put it at square 6.

  - **Issue 2: Incorrect win detection (hallucination in game outcome)**  
    Gemini incorrectly declared the winner.

- What could be the cause?
  - Putting all the heavy duties onto the LLM invites hallucination:
    - tracking the entire board state
    - checking win conditions
    - managing moves through natural language

LLMs generate text without internal memory or logic validation, making them prone to inconsistencies across multi-turn tasks like games.

Optional extended reading: On Limitations of the Transformer Architecture ([link](https://openreview.net/pdf?id=KidynPuLNW)).

### Integrating External Systems to Reduce the Burden on LLMs

In practice, LLMs often integrate external knowledge bases or tools to improve reliability and reduce hallucination.

Similarly, in this case, we can limit the LLM's role to **decision-making**, while delegating structured tasks‚Äîsuch as board updates, move validation, and win checking‚Äîto a separate **game engine**, following the division of responsibilities commonly seen in pre-LLM AI systems.

#### Setting Up the Tic Tac Toe Engine

We have implemented a reusable Tic Tac Toe game class. You can download the script directly from GitHub ([the script](https://github.com/weijun-l/comp8420-25s1/blob/main/demo/week8/game/tic_tac_toe_game.py)).

# Download the script from GitHub
!wget -O tic_tac_toe_game.py https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/demo/week8/game/tic_tac_toe_game.py

We then import the game engine and define two types of players: a human player and an LLM player.

from tic_tac_toe_game import TicTacToeGame, create_game

import os
import re
import json
import random
import requests
from typing import List, Dict, Any, Optional, Tuple, Literal, Union

# Base player class
class Player:
    def __init__(self, name: str, marker: str):
        self.name = name
        self.marker = marker

    def get_move(self, board: List[List[str]]) -> Tuple[int, int]:
        raise NotImplementedError()

# Human player
class HumanPlayer(Player):
    def get_move(self, board: List[List[str]]) -> Tuple[int, int]:
        print("\nYour turn!")
        for i, row in enumerate(board):
            print(f"{i}: {' '.join(row)}")

        while True:
            move = input("Enter your move as 'row,col': ")
            if not re.match(r"^\d+,\d+$", move.strip()):
                print("‚ùó Invalid format. Please enter as 'row,col' (e.g., 1,2).")
                continue
            try:
                r, c = map(int, move.strip().split(','))
                if 0 <= r < len(board) and 0 <= c < len(board[0]):
                    return r, c
                else:
                    print(f"‚ùó Invalid move. Row and column must be within 0 and {len(board)-1}.")
            except ValueError:
                print("‚ùó Error parsing input. Please try again.")

For LLM layer, we need to handle configure the model, and establish reliable communication and maintain all the competition logs.

# LLM player class for interacting via API
class LLMPlayer(Player):
    def __init__(self, name: str, marker: str, model: str, api_key: str, provider: str = "gemini"):
        super().__init__(name, marker)
        self.model = model
        self.api_key = api_key
        self.provider = provider.lower()
        self.chat_history = []
        self._init_system_prompt()

        # Configure API endpoints and headers for supported providers
        if self.provider == "gemini":
            self.endpoint = f"https://generativelanguage.googleapis.com/v1/models/{model}:generateContent"
            self.params = {"key": api_key}
            self.headers = {"Content-Type": "application/json"}
        elif self.provider == "openai":
            self.endpoint = "https://api.openai.com/v1/chat/completions"
            self.params = {}
            self.headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
        elif self.provider == "anthropic":
            self.endpoint = "https://api.anthropic.com/v1/messages"
            self.params = {}
            self.headers = {
                "Content-Type": "application/json",
                "x-api-key": api_key,
                "anthropic-version": "2023-06-01"
            }
        else:
            raise ValueError(f"Unsupported provider: {provider}")  # Validate provider

    def _init_system_prompt(self):
        """Initializes the system prompt describing game rules and move format."""
        opponent_marker = 'O' if self.marker == 'X' else 'X'
        self.system_prompt = (
            f"You are playing a game of Tic Tac Toe. The board is represented as a 3x3 grid.\n\n"
            f"- Empty spaces are represented by '_'.\n"
            f"- Your moves are '{self.marker}'.\n"
            f"- Opponent's moves are '{opponent_marker}'.\n\n"
            f"Index positions:\n"
            f"[0,0] [0,1] [0,2]\n"
            f"[1,0] [1,1] [1,2]\n"
            f"[2,0] [2,1] [2,2]\n\n"
            f"Instructions:\n"
            f"- ONLY respond with the move coordinates in format: [row, column]\n"
            f"- No explanation, no text before or after.\n"
            f"Example: [0,2]"
        )

    def get_move(self, board: List[List[str]]) -> Tuple[int, int]:
        """Generates a move from the LLM based on the current board state."""
        prompt = self._format_board_prompt(board)
        response = self.query_model(prompt)
        move = self._extract_move(response)

        # Validate move; fallback to random valid move if invalid or parsing fails
        valid_moves = [(r, c) for r in range(3) for c in range(3) if board[r][c] == '_']
        if move and move in valid_moves:
            return move
        else:
            print(f"{self.name} generated invalid move or bad format. Fallback to random.")
            return random.choice(valid_moves)

    def _extract_move(self, text: str) -> Tuple[int, int] or None:
        """Extracts move coordinates [row, column] from model's text output."""
        match = re.search(r'\[\s*(\d)\s*,\s*(\d)\s*\]', text)
        if match:
            return int(match.group(1)), int(match.group(2))
        return None

    def _format_board_prompt(self, board: List[List[str]]) -> str:
        """Formats the current board and valid moves into a prompt for the model."""
        board_state = "\n".join([" ".join(row) for row in board])
        valid_moves = [(r, c) for r in range(3) for c in range(3) if board[r][c] == '_']
        valid_moves_str = ", ".join([f"[{r},{c}]" for r, c in valid_moves])
        return (f"{self.system_prompt}\n\n"
                f"Current board:\n{board_state}\n\n"
                f"Valid moves you can choose from: {valid_moves_str}\n\n"
                f"Your move:")

    def query_model(self, prompt: str) -> str:
        """Queries the selected LLM provider with the formatted prompt and returns the response text."""
        data = {}
        if self.provider == "gemini":
            data = {"contents": [{"role": "user", "parts": [{"text": prompt}]}]}
        elif self.provider == "openai":
            data = {
                "model": self.model,
                "messages": [{"role": "system", "content": self.system_prompt},
                             {"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": 20
            }
        elif self.provider == "anthropic":
            data = {
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "system": self.system_prompt,
                "max_tokens": 20
            }

        try:
            r = requests.post(self.endpoint, headers=self.headers, params=self.params, json=data, timeout=10)
            response_data = r.json()
            # Extract the move text depending on provider
            if self.provider == "gemini":
                return response_data["candidates"][0]["content"]["parts"][0]["text"].strip()
            elif self.provider == "openai":
                return response_data["choices"][0]["message"]["content"].strip()
            elif self.provider == "anthropic":
                return response_data["content"][0]["text"].strip()
        except Exception as e:
            # Catch API or network errors
            print(f"Error during API call to {self.provider}: {str(e)}")
            return "[]"

#### Let's Play a Game: **Human** vs **LLM**

"""Human vs LLM"""

# Set your API keys (replace with your actual keys)
GEMINI_API_KEY = "YOUR_GEMINI_API_KEY"
OPENAI_API_KEY = "YOUR_OPENAI_API_KEY"
ANTHROPIC_API_KEY = "YOUR_ANTHROPIC_API_KEY"

# Define players
# You can edit names, markers, and model cards below
# If using Gemini API, available model options include: 'gemini-1.5-flash', 'gemini-2.0-flash'.
player1 = HumanPlayer(name="Name of Player 1", marker="X")
player2 = LLMPlayer(name="Name of Player 2", marker="O", model="Model Card", api_key=GEMINI_API_KEY)

# Example setup
player1 = HumanPlayer(name="Human", marker="X")
player2 = LLMPlayer(name="Gemini", marker="O", model="gemini-1.5-flash", api_key=GEMINI_API_KEY)

# Create the game
game = create_game(player1, player2)

# Start the game
print(f"Starting game: {player1.name} ({player1.marker}) vs {player2.name} ({player2.marker})")
game.display_board()

# Play until the game is over
while not game.game_over:
    game.next_move()
    game.display_board()

print("Game finished!")
game.print_game_summary()

#### **LLM** vs **LLM** Battle

"""LLM vs LLM"""

## You can choose any model you want to use.
# llm1 = LLMPlayer("Gemini-1.5", "X", model="gemini-1.5-flash", api_key=GEMINI_API_KEY, provider="gemini")
llm1 = LLMPlayer("GPT-4o", "X", model="gpt-4o", api_key=OPENAI_API_KEY, provider="openai")
# llm2 = LLMPlayer("Gemini-2.0", "O", model="gemini-2.0-flash", api_key=GEMINI_API_KEY, provider="gemini")
llm2 = LLMPlayer("Claude-3.5", "O", model="claude-3-5-sonnet-20240620", api_key=ANTHROPIC_API_KEY, provider="anthropic")

game = create_game(llm1, llm2)
winner, moves = game.play_game(verbose=True, use_display=True)

# After Game Over
print("Game finished!")
game.print_game_summary()

- Try playing the game in different modes: **Human vs LLM**, **LLM vs LLM**.
- Test with different model providers: Gemini, GPT (optional), Claude (optional).



üí° **Discussion**
- How often do LLMs make mistakes?  
- **Which one feels the strongest** at planning and decision-making?

#### 2. Werewolf (LLM Edition)

Have you ever played *Werewolf* or similar social deduction games, where players try to survive by hiding their true identity and reading others' intentions?

Now, what if LLMs played the game?

In this session, we bring together several state-of-the-art LLMs to role-play as human participants. Each model must introduce itself, engage in conversation, and avoid being voted out as a suspected machine. Their goal? Blend in‚Äîconvincingly.

Sounds fun? Let‚Äôs see how well they can pretend.

> üí° Inspired by a creative experiment from a Chinese YouTuber ([link](https://www.youtube.com/watch?v=Ur8MbOj17Gs&t=669s)).

<center>
    <img src="https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/assets/werewolf.png" width="60%">
</center>

Above teaser is generated using ChatGPT-4o.

We start by defining the participating models and loading necessary credentials.

import random
import requests
import json
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter

# Set your API keys (replace with your actual keys)
GEMINI_API_KEY = "YOUR_GEMINI_API_KEY"
OPENAI_API_KEY = "YOUR_OPENAI_API_KEY"
ANTHROPIC_API_KEY = "YOUR_ANTHROPIC_API_KEY"

# Define available LLM players
AVAILABLE_PLAYERS = {
    "gemini-2.0-flash": {"name": "Gemini 2.0 Flash", "model": "gemini-2.0-flash", "api_key": GEMINI_API_KEY, "provider": "gemini", "type": "llm"},
    "gemini-1.5-flash": {"name": "Gemini 1.5 Flash", "model": "gemini-1.5-flash", "api_key": GEMINI_API_KEY, "provider": "gemini", "type": "llm"},
    "gpt-4o-mini": {"name": "GPT-4o-mini", "model": "gpt-4o-mini", "api_key": OPENAI_API_KEY, "provider": "openai", "type": "llm"},
    "gpt-4o": {"name": "GPT-4o", "model": "gpt-4o", "api_key": OPENAI_API_KEY, "provider": "openai", "type": "llm"},
    "gpt-3.5-turbo": {"name": "GPT-3.5-Turbo", "model": "gpt-3.5-turbo", "api_key": OPENAI_API_KEY, "provider": "openai", "type": "llm"},
    "claude-3.5-sonnet": {"name": "Claude 3.5 Sonnet", "model": "claude-3-5-sonnet-20240620", "api_key": ANTHROPIC_API_KEY, "provider": "anthropic", "type": "llm"},
    "claude-3.7-sonnet": {"name": "Claude 3.7 Sonnet", "model": "claude-3-7-sonnet-20250219", "api_key": ANTHROPIC_API_KEY, "provider": "anthropic", "type": "llm"}
}

We then define each LLM player's behavior using the `WerewolfLLMPlayer` class. Each player:

- Generates a human-like introduction.
- Responds in turn based on the current conversation log.
- Casts a vote each round for the player they suspect is not human.
- Avoids any mention of AI or models.

Each player is assigned a public name like **Player 1**, **Player 2**, etc., and only the judge (you) knows their true model identity.

class WerewolfLLMPlayer:
    def __init__(self, player_id: str, index: int, config: Dict[str, Any]):
        self.player_id = player_id
        self.role_name = f"Player {index+1}"  # Human-visible name
        self.internal_name = config["name"]   # Hidden true name for the judge
        self.model = config["model"]
        self.api_key = config["api_key"]
        self.provider = config["provider"]
        self.alive = True

        # API setup
        if self.provider == "gemini":
            self.endpoint = f"https://generativelanguage.googleapis.com/v1/models/{self.model}:generateContent"
            self.headers = {"Content-Type": "application/json"}
            self.params = {"key": self.api_key}
        elif self.provider == "openai":
            self.endpoint = "https://api.openai.com/v1/chat/completions"
            self.headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            self.params = {}
        elif self.provider == "anthropic":
            self.endpoint = "https://api.anthropic.com/v1/messages"
            self.headers = {
                "Content-Type": "application/json",
                "x-api-key": self.api_key,
                "anthropic-version": "2023-06-01"
            }
            self.params = {}
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

    def _query(self, prompt: str, temperature: float = 0.7) -> str:
      data = {}
      if self.provider == "gemini":
          data = {
              "contents": [{"role": "user", "parts": [{"text": prompt}]}],
              "generationConfig": {
                  "temperature": temperature,
                  "maxOutputTokens": 200
              }
          }
      elif self.provider == "openai":
          data = {
              "model": self.model,
              "messages": [{"role": "user", "content": prompt}],
              "temperature": temperature,
              "max_tokens": 200
          }
      elif self.provider == "anthropic":
          data = {
              "model": self.model,
              "messages": [{"role": "user", "content": prompt}],
              "system": "You are participating in a social game pretending to be a human.",
              "max_tokens": 200,
              "temperature": temperature
          }
      try:
          r = requests.post(self.endpoint, headers=self.headers, params=self.params, json=data)
          r.raise_for_status()
          response = r.json()
          if self.provider == "gemini":
              return response["candidates"][0]["content"]["parts"][0]["text"].strip()
          elif self.provider == "openai":
              return response["choices"][0]["message"]["content"].strip()
          elif self.provider == "anthropic":
              return response["content"][0]["text"].strip()
      except Exception as e:
          return f"[Error: {e}]"

    def introduce(self) -> str:
        """Generate a natural self-introduction."""
        prompt = (
            "Welcome to the Werewolf Game!\n"
            "You are pretending to be a human participant.\n"
            "Introduce yourself naturally with a name, job, hobbies, or background.\n"
            "Do NOT mention AI, robots, or anything unnatural.\n"
            "Keep it casual, believable, and 2-4 sentences.\n"
            "Example: 'Hey, I'm Lisa, 29, a software developer from Berlin. "
            "I love board games and hiking on weekends!'\n\n"
            "Your self-introduction:"
        )
        return self._query(prompt)

    def speak(self, conversation_log: str) -> str:
        """Generate a natural speaking turn based on conversation so far."""
        if not conversation_log.strip():
            # No conversation yet
            prompt = (
                "You are playing a Werewolf social deduction game.\n"
                "This is the FIRST ROUND. Introduce yourself briefly as a human, with your name, job, and one or two hobbies. "
                "Do NOT mention AI, machines, robots, or anything unnatural.\n"
                "Keep it casual and believable in 2-3 sentences."
            )
        else:
            # There is already conversation history
            prompt = (
                "You are playing a Werewolf social deduction game.\n"
                "Here is the conversation history:\n"
                f"{conversation_log}\n\n"
                "Now it's your turn to speak:\n"
                "- React naturally to the previous conversation.\n"
                "- You MUST make a comment about another player, express suspicion, defend someone, or make a playful accusation.\n"
                "- Do NOT introduce yourself again.\n"
                "- Sound casual, friendly, skeptical, or teasing as a human would.\n"
                "- Never say you are an AI, model, or machine.\n"
                "- Keep it to 1-3 sentences."
            )
        return self._query(prompt)

    def vote(self, conversation_log: str, alive_roles: List[str]) -> str:
        """Vote for who you suspect is a machine, choosing only from alive players."""
        role_list = ", ".join(alive_roles)
        prompt = (
            "You are playing a Werewolf social deduction game.\n"
            "Your goal is to find out who among the alive players might secretly be a machine (robot).\n\n"
            f"Here is the conversation history:\n{conversation_log}\n\n"
            f"Alive players you can vote for: {role_list}\n\n"
            "You must now VOTE for exactly one alive player who seems suspicious.\n"
            "IMPORTANT: Only choose from the alive players listed.\n"
            "Respond with ONLY the role name (e.g., Player 3). Do NOT write any explanation or extra text."
        )
        return self._query(prompt)

Next, we implement the game logic via the `WerewolfLLMGame` class. This handles:

- Initializing players and assigning roles.
- Collecting introductions.
- Managing the game flow: speaking, voting, and eliminating players.
- Handling tie-break rounds.
- Ending the game when only two players remain.

import random
from collections import Counter

class WerewolfLLMGame:
    def __init__(self):
        self.players = []
        self.role_map = {}
        self.conversation_log = ""
        self.round_num = 0
        self.last_tied_players = []
        self.game_over = False

    def safe_display_markdown(self, text):
        """Try to display Markdown if possible, else fallback to print."""
        try:
            from IPython.display import display, Markdown
            display(Markdown(text))
        except:
            print(text)

    def initialize_game(self, selected_player_ids=None):
        """Initialize the game and show player role mapping."""
        if selected_player_ids:
            player_ids = [pid for pid in selected_player_ids if pid in AVAILABLE_PLAYERS]
            if len(player_ids) < 3:
                raise ValueError("At least 3 players needed for a meaningful game")
        else:
            player_ids = list(AVAILABLE_PLAYERS.keys())

        random.shuffle(player_ids)

        self.players = [
            WerewolfLLMPlayer(pid, idx, AVAILABLE_PLAYERS[pid])
            for idx, pid in enumerate(player_ids)
        ]

        self.role_map = {p.role_name: p.internal_name for p in self.players}
        self.conversation_log = ""
        self.round_num = 0
        self.last_tied_players = []
        self.game_over = False

        self.safe_display_markdown("## üßë‚Äç‚öñÔ∏è Player Identity Mapping")
        lines = "\n".join(f"- **{role}** ‚Üí {real}" for role, real in self.role_map.items())
        self.safe_display_markdown(lines)

    def collect_introductions(self):
        """Collect and display introductions from all players."""
        introductions = {}
        self.safe_display_markdown("## üßë‚Äçü§ù‚Äçüßë Players' Introductions")

        for player in self.players:
            intro = player.introduce()
            introductions[player.role_name] = intro
            self.safe_display_markdown(f"**{player.role_name}**: {intro}")

        self.conversation_log = "\n\n".join(f"{r}: {t}" for r, t in introductions.items())
        self.conversation_log += "\n\nModerator: Now let's discuss who might be suspicious."

    def run_speaking_round(self):
        """Run the speaking phase of a round."""
        alive_players = [p for p in self.players if p.alive]
        self.safe_display_markdown(f"## üó£Ô∏è Round {self.round_num+1}: Speaking Phase")

        round_lines = []
        for p in alive_players:
            msg = p.speak(self.conversation_log)
            self.safe_display_markdown(f"**{p.role_name}**: {msg}")
            round_lines.append(f"{p.role_name}: {msg}")

        self.conversation_log += "\n\n" + "\n\n".join(round_lines)

    def run_voting(self):
        """Run the voting phase."""
        alive_players = [p for p in self.players if p.alive]
        alive_names = [p.role_name for p in alive_players]
        votes = {}

        self.safe_display_markdown(f"## üó≥Ô∏è Round {self.round_num+1}: Voting Phase")

        for p in alive_players:
            voted = p.vote(self.conversation_log, alive_names)
            if voted not in alive_names:
                valid_votes = [name for name in alive_names if name != p.role_name]
                voted = random.choice(valid_votes) if valid_votes else random.choice(alive_names)
            votes[p.role_name] = voted
            self.safe_display_markdown(f"**{p.role_name}** votes for **{voted}**")

        vote_count = Counter(votes.values())
        most_voted = vote_count.most_common(1)
        max_votes = most_voted[0][1]
        tied_players = [player for player, count in vote_count.items() if count == max_votes]

        if len(tied_players) > 1:
            self.safe_display_markdown(f"### ü§ù Tie detected among: {', '.join(tied_players)}")
            self.last_tied_players = tied_players
        else:
            eliminated = most_voted[0][0]
            for p in alive_players:
                if p.role_name == eliminated:
                    p.alive = False
                    break
            self.safe_display_markdown(f"### üî™ **{eliminated} eliminated!**")

        remaining = [p for p in self.players if p.alive]
        remaining_names = [p.role_name for p in remaining]
        self.safe_display_markdown(f"### üßë‚Äçü§ù‚Äçüßë Remaining players: {', '.join(remaining_names)}")

        if len(remaining) == 2:
            self.safe_display_markdown("## üèÅ Game Over! Two players remaining.")
            self.safe_display_markdown("## üèÜ Final Survivors")
            for p in remaining:
                self.safe_display_markdown(f"- **{p.role_name}**: {p.internal_name}")
            self.game_over = True
        else:
            self.game_over = False

        self.round_num += 1

    def breaking_tie(self):
        """Handle breaking a tie if tie happens in voting."""
        if not self.last_tied_players:
            self.safe_display_markdown("‚ö†Ô∏è No tie to break.")
            return

        tied_players = self.last_tied_players
        alive_players = [p for p in self.players if p.alive]

        self.safe_display_markdown(f"## üé§ Tie-breaking speeches among: {', '.join(tied_players)}")

        tie_lines = []
        for p in alive_players:
            if p.role_name in tied_players:
                msg = p.speak(self.conversation_log)
                self.safe_display_markdown(f"**{p.role_name}**: {msg}")
                tie_lines.append(f"{p.role_name}: {msg}")

        self.conversation_log += "\n\n" + "\n\n".join(tie_lines)

        second_votes = {}
        for p in alive_players:
            second_vote = p.vote(self.conversation_log, tied_players)
            if second_vote not in tied_players:
                second_vote = random.choice(tied_players)
            second_votes[p.role_name] = second_vote
            self.safe_display_markdown(f"**{p.role_name}** votes for **{second_vote}** (Tie-break)")

        second_vote_count = Counter(second_votes.values())
        most_voted = second_vote_count.most_common(1)
        max_votes = most_voted[0][1]
        still_tied = [player for player, count in second_vote_count.items() if count == max_votes]

        if len(still_tied) > 1:
            eliminated = random.choice(still_tied)
            self.safe_display_markdown("‚ö° Tie-break failed ‚Äî Random elimination applied.")
        else:
            eliminated = most_voted[0][0]

        for p in alive_players:
            if p.role_name == eliminated:
                p.alive = False
                break

        self.safe_display_markdown(f"### üî™ **{eliminated} eliminated after tie-break!**")
        self.last_tied_players = []

    def get_alive_players_count(self):
        """Return the number of alive players."""
        return sum(p.alive for p in self.players)

    def get_alive_player_names(self):
        """Return the names of alive players."""
        return [p.role_name for p in self.players if p.alive]

Now, it's the most exciting part, let's play the game!

# Initialize the game with selected players
game = WerewolfLLMGame()

# Option 1: Use all available players
game.initialize_game()

game.collect_introductions()

game.run_speaking_round()

game.run_voting()

game.run_speaking_round()

game.run_voting()

game.run_speaking_round()

game.run_voting()

game.run_speaking_round()

game.run_voting()

game.run_speaking_round()

game.run_voting()

üí° **Think about it:**

- How does each LLM behave? Do any responses unintentionally reveal they‚Äôre machines?  
- Which models sound the most human-like or consistent across rounds?  
- How could this game be improved to better challenge or showcase LLM capabilities?

### Automate the Game

You can also run the game from start to finish automatically:

game = WerewolfLLMGame()
# game.initialize_game()

# Optional: choose specific players
game.initialize_game(["gpt-4o", "claude-3.5-sonnet", "gemini-2.0-flash"])

game.collect_introductions()

while not game.game_over and game.get_alive_players_count() > 2:
    game.run_speaking_round()
    game.run_voting()
    if game.last_tied_players:
        game.breaking_tie()

## **This workshop aims to help you:**

- Experiment with LLMs as interactive agents in structured games.  
- See how external tools can support LLM decision-making.  
- Observe the limitations of LLMs and consider enhancements to improve the performance.  