# COMP8420 Advanced Natural Language Processing
## Week 2 - Deep Learning, PyTorch & HuggingFace Hub

## Today's Topics
- Deep Learning & PyTorch
- Multi-layer Perceptron (MLP) Model
- HuggingFace Hub
  - Datasets
  - Pre-trained Model & Pipeline
  - Transformer Trainer


### 1. Deep Learning & PyTorch

Deep learning is a type of machine learning that uses **Artificial Neural Networks** to learn from data.

Deep learning algorithms learn to associate features in the data ($x$) with the correct labels ($y$).

The learning process typically consists of two steps:
>- **Feedforward**: feed the data into the model to calculate the prediction, and obtain the error with ground truth.
>- **Backpropagation**: backpropagate the error to all parameters involved in the feedforward path to get the gradients, and update the parameters.

For example, below image shows the learning process of a **Perceptron** network.

<center>
    <img src="https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/assets/perceptron.png" width="50%">
</center>

The learning process can be complex even for a simple network, as we need to calculate the derivative for every parameter manually.

Thanks to many deep learning frameworks, such as PyTorch, the Neural Network training process can be much **simpler** and more **efficient**.

[PyTorch](https://pytorch.org/) is a Python package that provides two high-level features:

- **Deep neural networks** built on an automatic differentiation system (autograd)
  - Automatically computes gradients and performs backpropagation for model training
  - Makes it easier to build and train complex neural networks

- **Tensor computation** (like NumPy) with strong **GPU acceleration**
  - Think of tensors as multi-dimensional arrays that can run on GPUs
  - Enables fast computation for large-scale data processing

Let's try our Perceptron example using PyTorch.

"""Feedforward with PyTorch Neural Network class"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np

# Define the Perceptron as a Neural Network class
class Perceptron(nn.Module):
    def __init__(self, input_size):
        super(Perceptron, self).__init__()
        self.linear = nn.Linear(input_size, 1)  # Linear layer (includes weights and bias)
        self.relu = nn.ReLU()                   # ReLU activation

    def forward(self, x):                       # Feedforward process
        out = self.linear(x)
        out = self.relu(out)
        return out

# Create input data and target
X = torch.tensor([1.0, 2.0])
y = torch.tensor([1.0])

# Q1: How can we create a model instance?
model = Perceptron(input_size=2)

# Initialize weights and bias
w_old = [0.1, 0.2]
b_old = 0.1
model.linear.weight.data = torch.tensor([[w_old[0], w_old[1]]])
model.linear.bias.data = torch.tensor([b_old])

criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

# Forward pass
y_pred = model(X)
loss = 0.5 * criterion(y_pred, y)

print(f"Output: {y_pred.item()}")
print(f"Mean Squared Error using PyTorch's nn.MSELoss: L = {loss.item()}")

"""Backpropagation with PyTorch"""

optimizer.zero_grad()  # Clear previous gradients
loss.backward()        # Calculate gradients
optimizer.step()       # Update parameters

# Get gradients and new values
w_grad = model.linear.weight.grad[0].tolist()
b_grad = model.linear.bias.grad.item()

w_new = model.linear.weight[0].tolist()
b_new = model.linear.bias.item()

# Create DataFrame to display results
params_df = pd.DataFrame({
    'Parameter': ['w1', 'w2', 'b'],
    'Old Value': [w_old[0], w_old[1], b_old],
    'Gradient': [w_grad[0], w_grad[1], b_grad],
    'New Value': [w_new[0], w_new[1], b_new]
})

print("\nUpdated Parameters:")
params_df

### 2. Multi-layer Perceptron (MLP) Model


We can also use PyTorch to build neural networks for many tasks.

In this tutorial, we will:
* Create a simple Multi-layer Perceptron (MLP) neural network
* Use it to analyze if movie reviews are positive or negative
* Work with the [Stanford Sentiment Treebank v2 (SST-2)](https://huggingface.co/datasets/stanfordnlp/sst2) dataset from Hugging Face

!pip install datasets

First, let's load the dataset from HuggingFace.

"""Load datasets from HuggingFace"""

from datasets import load_dataset
import random
import pandas as pd

random.seed(42)

dataset = load_dataset("stanfordnlp/sst2")
train_data = dataset["train"]

indices = random.sample(range(len(train_data)), 2000)

sentences = [train_data[i]["sentence"] for i in indices]
labels = [train_data[i]["label"] for i in indices]

sst2_df = pd.DataFrame({
    "sentence": sentences,
    "label": labels
})

sst2_df.head()

Second, we can use NLTK to pre-process the loaded datasets.

"""Data pre-processing"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

nltk.download(['punkt_tab', 'stopwords', 'wordnet'])

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Tokenize
    tokens = word_tokenize(text)
    # Q2: Try commenting out the remove stop-word step to check the change in the number of features.
    tokens = [word for word in tokens if word not in stop_words]
    # Q3: Try commenting out the lemmatization step to check the change in the number of features.
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

# Preprocess texts
print("Preprocessing texts...")
preprocessed_sentences = [preprocess_text(sentence) for sentence in sst2_df['sentence']]

Third, we continue to transform the data into numerical form using TF-IDF.

"""TF-IDF transformation"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(preprocessed_sentences).toarray()
y = np.array(sst2_df['label'])

feature_names = vectorizer.get_feature_names_out()
print(f"Feature dimension: {len(feature_names)}")

start_idx=200
end_idx=start_idx+5
print(f"Selected feature names: {feature_names[start_idx:end_idx]}")

print("\nSample feature vector:")
# Q4: Please print and check one feature vector sample, e.g., the first sample of X.
# np.set_printoptions(threshold=np.inf)
# np.set_printoptions(threshold=100)
print(X[0])

Next, we split the dataset to get **train**, **validation** and **test** sets.

from sklearn.model_selection import train_test_split

# First, split into train (800) and temp (200)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=200, random_state=42, stratify=y
)

# Then split temp into validation (100) and test (100)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print(f"\nTraining set size: {X_train.shape[0]}, Validation set size: {X_val.shape[0]}, Test set size: {X_test.shape[0]}")
print(f"Training set label distribution: {np.bincount(y_train)}")
print(f"Validation set label distribution: {np.bincount(y_val)}")
print(f"Test set label distribution: {np.bincount(y_test)}")

from torch.utils.data import Dataset, DataLoader

class TextDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return torch.FloatTensor(self.features[idx]), self.labels[idx]

train_dataset = TextDataset(X_train, y_train)
val_dataset = TextDataset(X_val, y_val)
test_dataset = TextDataset(X_test, y_test)

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

We will now build our MLP model for this task. The model can have different numbers of layers and sizes.    

Since we're using a small sample of data, we'll keep things simple with a 3-layer network.

import torch
import torch.nn as nn
import torch.optim as optim

class MLPClassifier(nn.Module):
    def __init__(self, input_dim):
        super(MLPClassifier, self).__init__()
        self.layer1 = nn.Linear(input_dim, 128)
        self.layer2 = nn.Linear(128, 64)
        self.layer3 = nn.Linear(64, 2)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.dropout(x)
        x = self.relu(self.layer2(x))
        x = self.dropout(x)
        output = self.layer3(x)
        return output

model = MLPClassifier(input_dim=X.shape[1])
criterion = nn.CrossEntropyLoss()

# Q5: We need to define what variables are we plan to updated during the training, what should we input?
optimizer = optim.Adam(model.parameters(), lr=0.001)

print("Model Architecture:")
print(model)
total_params = sum(p.numel() for p in model.parameters())
print(f"Total number of parameters: {total_params}")

ðŸ’¡ **Practice:**

Can you draw the structrue of the MLP model we have just created?

<center>
    <img src="https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/assets/mlp.png" width="50%">
</center>

We can then train the model on **training** data, and monitor the training and validation losses.

"""Training process for the MLP model"""

import torch
from tqdm import tqdm

num_epoch=3
for epoch in range(num_epoch):
    # Training phase
    model.train()
    train_loss = 0.0

    train_batches = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epoch} [Train]")
    for inputs, labels in train_batches:
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        train_batches.set_postfix(loss=loss.item())

    train_loss = train_loss / len(train_loader)

    # Validation phase
    model.eval()
    val_loss = 0.0

    val_batches = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epoch} [Valid]")
    with torch.no_grad():  # No gradient calculation for validation phase.
        for inputs, labels in val_batches:

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            val_batches.set_postfix(loss=loss.item())

    val_loss = val_loss / len(val_loader)

    print(f'Epoch {epoch+1}/{num_epoch}:')
    print(f'  Training Loss: {train_loss:.4f}')
    print(f'  Validation Loss: {val_loss:.4f}')

After training the model, we can evaluate its performance on test split data.

"""Evaluating the model on test data"""
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Evaluate on test set
model.eval()
all_predictions = []
all_labels = []

with torch.no_grad():
    for inputs, labels in test_loader:
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        all_predictions.extend(predicted.numpy())
        all_labels.extend(labels.numpy())

# Calculate accuracy
accuracy = accuracy_score(all_labels, all_predictions)
print(f'Test Set Accuracy: {accuracy * 100:.2f}%')

# Print classification report
print("\nClassification Report:")
print(classification_report(all_labels, all_predictions,
                           target_names=['Negative (0)', 'Positive (1)']))

After training the model, we can look at which words matter most by examining the model's weights. This helps us understand what the model learns from the data.

"""Check feature (word) importance based on the absolute weight values"""

def get_important_words(model, feature_names, top_n=10):
    """Get the most important words based on model weights"""
    weights = model.layer1.weight.data.numpy()
    print(f"The shape of weight parameters is {weights.shape}.")

    importance = np.abs(weights).mean(axis=0)
    top_indices = importance.argsort()[-top_n:][::-1]

    return [(feature_names[i], importance[i]) for i in top_indices]

# Get important features
important_words = get_important_words(model, feature_names)

print("\nMost Important Words:")
for word, importance in important_words:
    print(f"{word}: {importance:.4f}")

ðŸ’¡ **Reflection**
- TF-IDF and Bag-of-words turn sentences into long but mostly **empty (sparse) representations**.
- These methods treat each word separately and miss the meaning connections between words.
- Simple MLP networks start with no language knowledge and must learn everything from limited training data.

This is why modern NLP approaches using **pretrained models (especially Transformers)** come in.


### 4. HuggingFace <img src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg" width="70%">

[Hugging Face](https://huggingface.co/) is a collaboration platform for the machine learning community, providing open-source ML libraries, datasets, and models:

- Libraries: Transformers, Datasets, Trainer, and others
- Datasets: Open-source NLP (e.g., [GLUE benchmark](https://huggingface.co/datasets/nyu-mll/glue)) and Computer Vision datasets (e.g., [CIFAR-10](https://huggingface.co/datasets/uoft-cs/cifar10))
- Models: Pre-trained language models (e.g., Google [BERT](https://huggingface.co/google-bert/bert-base-uncased), OpenAI [GPT-2](https://huggingface.co/openai-community/gpt2), Meta [Llama](https://huggingface.co/meta-llama/Llama-2-7b-hf))

In today's workshop, we will mainly focus on finding and loading open-source datasets from the HuggingFace platform. We will use a text classification dataset - [Stanford Sentiment Treebank v2 (SST-2)](https://huggingface.co/datasets/stanfordnlp/sst2) as an example.

#### Datasets

NLP techniques are designed to deal with real-world tasks, such as:
- Text classification (e.g., movie/product sentiment analysis, email spam detection), such as [Stanford Sentiment Treebank v2 (SST-2)](https://huggingface.co/datasets/stanfordnlp/sst2).
- Question answering (e.g., customer service automation, document retrieval systems, medical diagnosis assistance), such as [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/rajpurkar/squad).
- Text generation (e.g., chatbots like ChatGPT, Gemini, Claude), such as [Stanford Alpaca](https://huggingface.co/datasets/yahma/alpaca-cleaned).

For more information about NLP tasks and pipelines, check out the [Hugging Face Pipeline documentation](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline.task).

A variety of datasets for each task are available on Hugging Face, allowing users to train models for specific tasks.

"""Load a Text classification dataset from Hugging Face"""

from pprint import pprint
sst2 = load_dataset("stanfordnlp/sst2")

pprint(sst2['train'][0], width=100)

"""Load a Question Answering dataset from Hugging Face"""

squad = load_dataset("squad")

pprint(squad['train'][0], width=100)

It's your turn, please visit [Hugging Face Dataset](https://huggingface.co/datasets), to find and load a dataset.

# Q6: Please find one dataset from HuggingFace and print one of its samples.

"""Example answer: load a text generation dataset."""

alpaca = load_dataset("yahma/alpaca-cleaned")

pprint(alpaca['train'][0], width=100)

#### Pre-trained Model & Pipeline

While we have access to various datasets, **we need language models to process this data and perform NLP tasks.**

Many organizations (Google, Meta AI, OpenAI) and researchers have shared their **pre-trained** and **fine-tuned models** on Hugging Face for free use.

Pretrained models learn **fundamental language knowledge** from massive text collections before being fine-tuned on specific tasks for better performance.

Let's load one pre-trained [model](https://huggingface.co/JeremiahZ/bert-base-uncased-sst2) from [HuggingFace](https://huggingface.co/models) and examine its structure.

"""Load a model from Hugging Face"""
from transformers import AutoModel

#### You can look up any models on Hugging Face and replace with the model card name ####
model_card = "JeremiahZ/bert-base-uncased-sst2"
#########################################################################################

model = AutoModel.from_pretrained(model_card)

"""Print model architecture"""
model

> **ðŸ’¡ Reflection**
> - Transformer models are a type of deep learning architecture. Can you connect what you observed about BERT's structure with your previous knowledge of deep learning models?
> - More details on Transformer models such as BERT will be covered in following lectures. You can also check out its [original paper](https://aclanthology.org/N19-1423/) and [implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py).

You can use these pre-trained models to work on specific tasks, even without knowing their detailed structure or training.

"""Example of text classification pipeline"""
from transformers import pipeline

cls_pipe = pipeline("text-classification", model="JeremiahZ/bert-base-uncased-sst2")

cls_pipe(["This restaurant is awesome", "This restaurant is awful"])

> **ðŸ’¡ Think About It:**
> - How do you interpret the labels and scores in the results?
> - Are these predictions reasonable?

# Q7: Feel free to input any sentence you would like to evaluate.

ask_anything="The weather today is comfortable."
cls_pipe(ask_anything)

There are also pipelines designed for other tasks, feel free to explore  [Hugging Face Pipeline documentation](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline.task).

### Transformer Trainer (Fine-tune Pre-trained Models) -- (Change Runtime to T4 GPU for Acceleration)

To work on a specific task, even if we can't find an already fine-tuned model, we can fine-tune a pre-trained model on our own dataset.

We'll use *tiny-bert* on the *SST-2* dataset as an example, using the Transformers Trainer API. This API makes it easy to handle training, optimization, and evaluation.

!pip install datasets

"""Dataset Preparation"""

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load dataset and model
sst2 = load_dataset("stanfordnlp/sst2")
tokenizer = AutoTokenizer.from_pretrained("huawei-noah/TinyBERT_General_4L_312D")
model = AutoModelForSequenceClassification.from_pretrained("huawei-noah/TinyBERT_General_4L_312D", num_labels=2)
model = model.to(device)

full_train = sst2["train"].shuffle(seed=42).select(range(2000))
train_size = int(0.8 * len(full_train))  # 80% for training
val_size = int(0.1 * len(full_train))    # 10% for validation

train_data = full_train.select(range(train_size))
dev_data = full_train.select(range(train_size, train_size + val_size))
test_data = full_train.select(range(train_size + val_size, len(full_train)))

def tokenize_function(examples):
    return tokenizer(examples["sentence"], padding="max_length", truncation=True, max_length=128, return_tensors="pt")

# Tokenize all datasets
train_dataset = train_data.map(tokenize_function, batched=True)
dev_dataset = dev_data.map(tokenize_function, batched=True)
test_dataset = test_data.map(tokenize_function, batched=True)

"""Fine-tune the Pretrained Model on Given data"""

from transformers import TrainingArguments, Trainer
import numpy as np

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=4,
    eval_strategy="epoch",
    report_to="none",
    disable_tqdm=False,
    logging_steps=5
)

# Define the metrics function
def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return {"accuracy": np.mean(predictions == eval_pred.label_ids)}

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    compute_metrics=compute_metrics,
)

# Start the training
trainer.train()

> **ðŸ’¡ Remark**
> - Here we override Hugging Face's Trainer by defining a custom `compute_metrics` function that calculates average accuracy across all samples.
> - The `eval_pred` parameter contains: `predictions` (model's logits), `label_ids` (ground truth), and optionally `inputs`, which are collected in the Trainer's [evaluation_loop](https://github.com/huggingface/transformers/blob/v4.25.1/src/transformers/trainer.py#L3090).
> - Without this override, evaluation would only report `eval_loss` since metrics default to an empty dictionary when `compute_metrics` is None.
> - For our override function, since `predictions` contain raw logits, we must use `np.argmax` to obtain predicted labels before comparing them to true labels for accuracy calculation.

"""Evaluate the fine-tuned model on test dataset"""

test_results = trainer.evaluate(test_dataset)
print(f"Test set accuracy: {test_results['eval_accuracy']:.3f}")

> ðŸ’¡ **Reflection:**
>
> - Hugging Face offers helpful packages (Transformers, Datasets, Trainer, Pipeline) that make your work easier, but they're still based on core deep learning concepts.
>   - Transformers: Ready-to-use neural networks with pretrained weights
>   - Datasets: User-friendly tools for managing and loading data
>   - Trainer: Simplified framework for model training and evaluation
>   - Pipeline: Easy-to-use tools for model inference

## **This workshop aims to help you:**

- Understand how deep learning works (Feedforward and Backpropagation)
- Build and train Neural Networks using PyTorch
- Learn to find datasets on the Hugging Face platform
- Use Pre-trained Models for specific tasks
- Fine-tune Pre-trained Models with the Trainer API