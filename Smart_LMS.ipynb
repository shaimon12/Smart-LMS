{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ddbc2a",
   "metadata": {},
   "source": [
    "# COMP8420 Student Agent – Dataset Preparation\n",
    "\n",
    "This notebook prepares the dataset for the Student Agent project. The dataset includes:\n",
    "\n",
    "1. Lecture content extracted from COMP8420 PDF slides  \n",
    "2. Practical files converted from Jupyter Notebooks (.ipynb)  \n",
    "3. All content is stored in plain `.txt` files to be used in a Retrieval-Augmented Generation (RAG) pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "153de806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: COMP8420-W6 - Dev LLMs - Fine-tuning.pdf → COMP8420-W6 - Dev LLMs - Fine-tuning.txt\n",
      "Extracted: COMP8420-W9-v2.pdf → COMP8420-W9-v2.txt\n",
      "Extracted: COMP8420-W4 - Use LLMs - Text processing.pdf → COMP8420-W4 - Use LLMs - Text processing.txt\n",
      "Extracted: COMP8420-W1.pdf → COMP8420-W1.txt\n",
      "Extracted: HEAL.pdf → HEAL.txt\n",
      "Extracted: NLP_Guest_Lecture_HMC_V2.pdf → NLP_Guest_Lecture_HMC_V2.txt\n",
      "Extracted: COMP8420-W13.pdf → COMP8420-W13.txt\n",
      "Extracted: COMP8420-W10.pdf → COMP8420-W10.txt\n",
      "Extracted: COMP8420-W7 - Und LLMs - Risk and future.pdf → COMP8420-W7 - Und LLMs - Risk and future.txt\n",
      "Extracted: COMP8420-W8 - Dev LLMs - Humanoid AI - 2.pdf → COMP8420-W8 - Dev LLMs - Humanoid AI - 2.txt\n",
      "Extracted: COMP8420-W5 - Dev LLMs - Multimodal LLMs.pdf → COMP8420-W5 - Dev LLMs - Multimodal LLMs.txt\n",
      "Extracted: COMP8420-W11-v2.pdf → COMP8420-W11-v2.txt\n",
      "Extracted: COMP8420-W2-v1.pdf → COMP8420-W2-v1.txt\n",
      "Extracted: COMP8420-W3 - Und LLMs - Foundation models.pdf → COMP8420-W3 - Und LLMs - Foundation models.txt\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your input/output paths\n",
    "pdf_folder = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Lectures\")  # Change this\n",
    "output_folder = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/lectures\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Extract each lecture PDF into a .txt file\n",
    "for pdf_file in pdf_folder.glob(\"*.pdf\"):\n",
    "    loader = PyMuPDFLoader(str(pdf_file))\n",
    "    documents = loader.load()\n",
    "    full_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "    txt_filename = pdf_file.stem + \".txt\"\n",
    "    with open(output_folder / txt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "    print(f\"Extracted: {pdf_file.name} → {txt_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e5ee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted: COMP8420-week3-solution.ipynb → COMP8420-week3-solution.txt\n",
      "Converted: COMP8420-week4-solution.ipynb → COMP8420-week4-solution.txt\n",
      "Converted: COMP8420-week5-solution.ipynb → COMP8420-week5-solution.txt\n",
      "Converted: COMP8420-week2-solution.ipynb → COMP8420-week2-solution.txt\n",
      "Converted: COMP8420-week4-practice.ipynb → COMP8420-week4-practice.txt\n",
      "Converted: COMP8420-week7-solution.ipynb → COMP8420-week7-solution.txt\n",
      "Converted: COMP8420-week3-practice.ipynb → COMP8420-week3-practice.txt\n",
      "Converted: COMP8420-week6-solution.ipynb → COMP8420-week6-solution.txt\n",
      "Converted: COMP8420_week1_solution.ipynb → COMP8420_week1_solution.txt\n",
      "Converted: COMP8420_week12_solution.ipynb → COMP8420_week12_solution.txt\n",
      "Converted: COMP8420_week9_solution.ipynb → COMP8420_week9_solution.txt\n",
      "Converted: COMP8420_week8_solution.ipynb → COMP8420_week8_solution.txt\n",
      "Converted: COMP8420_week11_solution.ipynb → COMP8420_week11_solution.txt\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "# Define paths for notebooks and output .txt\n",
    "prac_folder = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Prac\")  # Folder with your .ipynb files\n",
    "output_folder = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/practicals\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to extract markdown + code\n",
    "def extract_notebook_text(nb_path):\n",
    "    nb = nbformat.read(open(nb_path, \"r\", encoding=\"utf-8\"), as_version=4)\n",
    "    content = []\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type in ['markdown', 'code']:\n",
    "            content.append(cell.source)\n",
    "    return \"\\n\\n\".join(content)\n",
    "\n",
    "# Convert all .ipynb files to .txt\n",
    "for nb_file in prac_folder.glob(\"*.ipynb\"):\n",
    "    text = extract_notebook_text(nb_file)\n",
    "    txt_name = nb_file.stem + \".txt\"\n",
    "    with open(output_folder / txt_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"Converted: {nb_file.name} → {txt_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33752cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qna.json saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define Q&A dataset\n",
    "qa_data = [\n",
    "    {\n",
    "        \"question\": \"What is a foundation model in NLP?\",\n",
    "        \"answer\": \"A foundation model is a large pre-trained model trained on massive datasets, serving as the base for fine-tuning on specific NLP tasks.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are examples of foundation models?\",\n",
    "        \"answer\": \"Examples include GPT-3.5, Claude, PaLM, BERT, LLaMA, and Mistral.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does fine-tuning mean in the context of LLMs?\",\n",
    "        \"answer\": \"Fine-tuning is the process of continuing training on a pre-trained model using a smaller, task-specific dataset.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between prompt tuning and fine-tuning?\",\n",
    "        \"answer\": \"Prompt tuning adjusts input formatting without altering the model, while fine-tuning retrains the model on new data.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was covered in Week 6 of COMP8420?\",\n",
    "        \"answer\": \"Week 6 focused on fine-tuning large language models, covering parameter-efficient fine-tuning and adapter-based methods.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When is the COMP8420 project presentation due?\",\n",
    "        \"answer\": \"The presentation is scheduled for Week 13, Friday, June 6th, 2025.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the final deadline for Assignment 3?\",\n",
    "        \"answer\": \"Assignment 3 (code + report) is due during the exam period on June 17th, 2025.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What type of model should we use for our project?\",\n",
    "        \"answer\": \"You can use OpenAI’s GPT-3.5 or open-source models like LLaMA2 or Mistral, depending on your goals and data.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do we evaluate our NLP project?\",\n",
    "        \"answer\": \"Use metrics like BLEU, ROUGE, accuracy, and ablation studies to compare performance against baselines or alternatives.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are embedding models used for?\",\n",
    "        \"answer\": \"They convert text into vector form for similarity search and retrieval, commonly used in RAG pipelines.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do we retrieve answers in a RAG system?\",\n",
    "        \"answer\": \"Text chunks are embedded into vectors and searched via similarity to retrieve relevant content for generation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are lectures delivered in COMP8420?\",\n",
    "        \"answer\": \"Lectures are delivered via PDFs and practical notebooks, combining theory and hands-on exercises.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What technologies are used in this course?\",\n",
    "        \"answer\": \"Hugging Face, PyTorch, LangChain, OpenAI APIs, and vector databases like FAISS.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can we use ChatGPT in our project?\",\n",
    "        \"answer\": \"Yes, but your project must demonstrate additional engineering beyond just using the API.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is parameter-efficient fine-tuning?\",\n",
    "        \"answer\": \"It refers to fine-tuning techniques like LoRA and Adapters that update only a small subset of the model.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What should we cover in the presentation?\",\n",
    "        \"answer\": \"Your project title, real-world problem, methodology, expected outcome, and team contributions.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What’s the length of the presentation?\",\n",
    "        \"answer\": \"You must present for 3–4 minutes during the Week 13 Practice Workshop.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How can we improve our mark?\",\n",
    "        \"answer\": \"Make your project novel, apply evaluation methods, and clearly explain your work in the report and presentation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are some real-world NLP tasks?\",\n",
    "        \"answer\": \"Text classification, summarization, question answering, entity recognition, translation, and dialogue generation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can we build an agent using LangChain?\",\n",
    "        \"answer\": \"Yes, LangChain is commonly used to implement RAG-based agents using LLMs and vector stores.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save as qna.json\n",
    "with open(output_dir / \"qna.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_data, f, indent=2)\n",
    "\n",
    "print(\"qna.json saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c49ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deadlines.json saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Output directory (same as before)\n",
    "output_dir = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420\")\n",
    "\n",
    "# Define the deadlines\n",
    "deadlines = [\n",
    "    {\n",
    "        \"task\": \"Team Registration\",\n",
    "        \"due_date\": \"2025-05-30\",\n",
    "        \"type\": \"Workshop\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"Project Presentation (Week 13)\",\n",
    "        \"due_date\": \"2025-06-06\",\n",
    "        \"type\": \"Practice Workshop\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"Final Report + Code Submission\",\n",
    "        \"due_date\": \"2025-06-17\",\n",
    "        \"type\": \"Exam Period\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_dir / \"deadlines.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(deadlines, f, indent=2)\n",
    "\n",
    "print(\"deadlines.json saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6786858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_info.txt saved successfully.\n"
     ]
    }
   ],
   "source": [
    "course_info_text = \"\"\"\n",
    "Course Title: COMP8420 – Advanced Natural Language Processing (S1 2025)\n",
    "\n",
    "Description:\n",
    "This course teaches students how to apply modern Natural Language Processing (NLP) techniques using large language models (LLMs). It focuses on real-world applications and responsible development practices. Topics include foundation models, prompt engineering, fine-tuning, RAG pipelines, privacy, security, and AI agent design.\n",
    "\n",
    "Teaching Team:\n",
    "- Dr. Qiongkai Xu (Lecturer)\n",
    "- Prof. Longbing Cao (Supervisor)\n",
    "- Mr. Weijun Li (TA)\n",
    "\n",
    "Key Technologies:\n",
    "- Hugging Face, PyTorch, OpenAI APIs, LangChain, FAISS\n",
    "\n",
    "Assessments:\n",
    "- Assignment 1: Text Classification\n",
    "- Assignment 2: Text Generation\n",
    "- Assignment 3: Team Project (Presentation + Code + Report)\n",
    "\n",
    "Objective:\n",
    "Prepare students to build and deploy intelligent NLP systems with ethical awareness and practical skills.\n",
    "\"\"\"\n",
    "\n",
    "# Save to .txt\n",
    "with open(output_dir / \"course_info.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(course_info_text.strip())\n",
    "\n",
    "print(\"course_info.txt saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8b515b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "announcements.json saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420\")\n",
    "\n",
    "# Announcements dataset\n",
    "announcements = [\n",
    "    {\n",
    "        \"title\": \"Assignment 3 – Presentation Reminder\",\n",
    "        \"content\": \"Don't forget to prepare a 3–4 minute talk about your project for the Week 13 workshop (Friday June 6th).\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Assignment 3 Submission\",\n",
    "        \"content\": \"Final code and report must be submitted by June 17th during the exam period. Late submissions will not be accepted without special consideration.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Week 7 Workshop Topic\",\n",
    "        \"content\": \"We'll explore risks and safety issues in LLMs. Please review the Week 7 slides before the workshop.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Week 10 Team Registration\",\n",
    "        \"content\": \"Make sure to form your group and register your project title by the Week 10 workshop.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save as JSON\n",
    "with open(output_dir / \"announcements.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(announcements, f, indent=2)\n",
    "\n",
    "print(\"announcements.json saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e57e0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discussions.json saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Sample discussion Q&A\n",
    "discussions = [\n",
    "    {\n",
    "        \"question\": \"Do we have to use LangChain for Assignment 3?\",\n",
    "        \"answer\": \"No, LangChain is optional. You can use any framework that supports RAG or LLM integration.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can we use ChatGPT API?\",\n",
    "        \"answer\": \"Yes, but remember that you must demonstrate your own engineering effort in addition to using the API.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is it okay to work solo on Assignment 3?\",\n",
    "        \"answer\": \"Projects should be completed in teams of two unless you’ve received special permission.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What’s the expected length of the presentation?\",\n",
    "        \"answer\": \"Each team should present for 3–4 minutes during the Week 13 practice workshop.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can we include public datasets in our project?\",\n",
    "        \"answer\": \"Yes, you may include public datasets as long as they’re relevant to your topic and cited properly.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save as JSON\n",
    "with open(output_dir / \"discussions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(discussions, f, indent=2)\n",
    "\n",
    "print(\"discussions.json saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78ef7773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-community in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (0.3.24)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.tar.gz (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.2/70.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: openai in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (1.75.0)\n",
      "Requirement already satisfied: tiktoken in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain) (0.3.64)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain-community) (3.11.10)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.19)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/shaimonrahman/anaconda3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Building wheels for collected packages: faiss-cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for faiss-cpu (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for faiss-cpu \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[12 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'faiss._swigfaiss' extension\n",
      "  \u001b[31m   \u001b[0m swigging faiss/faiss/python/swigfaiss.i to faiss/faiss/python/swigfaiss_wrap.cpp\n",
      "  \u001b[31m   \u001b[0m swig -python -c++ -Doverride= -doxygen -Ifaiss -I/private/var/folders/n9/zw6f9z6j7tbf93ys9qbgldpw0000gn/T/pip-build-env-ea7i9ihs/overlay/lib/python3.11/site-packages/numpy/_core/include -Ifaiss -I/usr/local/include -o faiss/faiss/python/swigfaiss_wrap.cpp faiss/faiss/python/swigfaiss.i\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/shaimonrahman/anaconda3/bin/swig\", line 5, in <module>\n",
      "  \u001b[31m   \u001b[0m     from swig import swig\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'swig'\n",
      "  \u001b[31m   \u001b[0m error: command '/Users/shaimonrahman/anaconda3/bin/swig' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for faiss-cpu\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build faiss-cpu\n",
      "\u001b[31mERROR: Could not build wheels for faiss-cpu, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-community faiss-cpu openai tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c272ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Total chunks embedded: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/zw6f9z6j7tbf93ys9qbgldpw0000gn/T/ipykernel_11441/2309739295.py:63: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import nbformat\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, JSONLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Set your base dataset path\n",
    "dataset_path = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420\")\n",
    "persist_path = dataset_path / \"chroma_store\"\n",
    "persist_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "all_chunks = []\n",
    "\n",
    "# 1. Load Lecture PDFs\n",
    "lecture_path = dataset_path / \"lectures\"\n",
    "for pdf_file in lecture_path.glob(\"*.pdf\"):\n",
    "    loader = PyMuPDFLoader(str(pdf_file))\n",
    "    docs = loader.load()\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# 2. Load Practical Notebooks (.ipynb)\n",
    "pracs_path = dataset_path / \"practicals\"\n",
    "for ipynb_file in pracs_path.glob(\"*.ipynb\"):\n",
    "    nb = nbformat.read(open(ipynb_file, \"r\", encoding=\"utf-8\"), as_version=4)\n",
    "    text = \"\"\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type in [\"markdown\", \"code\"]:\n",
    "            text += cell.source + \"\\n\\n\"\n",
    "    doc = Document(page_content=text, metadata={\"source\": ipynb_file.name})\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# 3. Load JSON Files (qna, deadlines, etc.)\n",
    "json_files = [\"qna.json\", \"deadlines.json\", \"announcements.json\", \"discussions.json\"]\n",
    "for json_file in json_files:\n",
    "    loader = JSONLoader(\n",
    "        file_path=str(dataset_path / json_file),\n",
    "        jq_schema=\".[]\",\n",
    "        text_content=False\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# 4. Load course_info.txt\n",
    "info_loader = TextLoader(str(dataset_path / \"course_info.txt\"))\n",
    "docs = info_loader.load()\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "all_chunks.extend(chunks)\n",
    "\n",
    "# 5. Embed and save to Chroma\n",
    "embedding = OpenAIEmbeddings(disallowed_special=())\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_chunks,\n",
    "    embedding=embedding,\n",
    "    persist_directory=str(persist_path)\n",
    ")\n",
    "vectorstore.persist()\n",
    "\n",
    "print(f\"Done. Total chunks embedded: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d667648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/zw6f9z6j7tbf93ys9qbgldpw0000gn/T/ipykernel_11441/4044690047.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=persist_path, embedding_function=embedding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Assignment 3 (code + report) is due during the exam period on June 17th, 2025. The final code and report must be submitted by that date. Late submissions will not be accepted without special consideration.\n",
      "\n",
      "Sources:\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/qna.json\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/announcements.json\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/discussions.json\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/qna.json\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load embedding and Chroma vector store\n",
    "embedding = OpenAIEmbeddings(disallowed_special=())\n",
    "persist_path = \"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/chroma_store\"\n",
    "vectorstore = Chroma(persist_directory=persist_path, embedding_function=embedding)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# Load chat model (you can use \"gpt-3.5-turbo\" or any model you prefer)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Set up the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "query = \"When is Assignment 3 due and how do we submit it?\"\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])\n",
    "\n",
    "print(\"\\nSources:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata.get(\"source\", \"Unknown\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13751edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
