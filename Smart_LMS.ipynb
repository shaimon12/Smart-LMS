{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6754d627",
   "metadata": {},
   "source": [
    "# COMP8420 Student Agent – Dataset Preparation\n",
    "\n",
    "This notebook prepares the dataset for the Student Agent project. The dataset includes:\n",
    "\n",
    "1. Lecture content extracted from COMP8420 PDF slides  \n",
    "2. Practical files converted from Jupyter Notebooks (.ipynb)  \n",
    "3. All content is stored in plain `.txt` files to be used in a Retrieval-Augmented Generation (RAG) pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ba97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: COMP8420-W6 - Dev LLMs - Fine-tuning.pdf → COMP8420-W6 - Dev LLMs - Fine-tuning.txt\n",
      "Extracted: COMP8420-W9-v2.pdf → COMP8420-W9-v2.txt\n",
      "Extracted: COMP8420-W4 - Use LLMs - Text processing.pdf → COMP8420-W4 - Use LLMs - Text processing.txt\n",
      "Extracted: COMP8420-W1.pdf → COMP8420-W1.txt\n",
      "Extracted: HEAL.pdf → HEAL.txt\n",
      "Extracted: NLP_Guest_Lecture_HMC_V2.pdf → NLP_Guest_Lecture_HMC_V2.txt\n",
      "Extracted: COMP8420-W13.pdf → COMP8420-W13.txt\n",
      "Extracted: COMP8420-W10.pdf → COMP8420-W10.txt\n",
      "Extracted: COMP8420-W7 - Und LLMs - Risk and future.pdf → COMP8420-W7 - Und LLMs - Risk and future.txt\n",
      "Extracted: COMP8420-W8 - Dev LLMs - Humanoid AI - 2.pdf → COMP8420-W8 - Dev LLMs - Humanoid AI - 2.txt\n",
      "Extracted: COMP8420-W5 - Dev LLMs - Multimodal LLMs.pdf → COMP8420-W5 - Dev LLMs - Multimodal LLMs.txt\n",
      "Extracted: COMP8420-W11-v2.pdf → COMP8420-W11-v2.txt\n",
      "Extracted: COMP8420-W2-v1.pdf → COMP8420-W2-v1.txt\n",
      "Extracted: COMP8420-W3 - Und LLMs - Foundation models.pdf → COMP8420-W3 - Und LLMs - Foundation models.txt\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your input/output paths\n",
    "pdf_folder = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Lectures\")  # Change this\n",
    "output_folder = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/lectures\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Extract each lecture PDF into a .txt file\n",
    "for pdf_file in pdf_folder.glob(\"*.pdf\"):\n",
    "    loader = PyMuPDFLoader(str(pdf_file))\n",
    "    documents = loader.load()\n",
    "    full_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "    txt_filename = pdf_file.stem + \".txt\"\n",
    "    with open(output_folder / txt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "    print(f\"Extracted: {pdf_file.name} → {txt_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "108f12d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted: COMP8420-week3-solution.ipynb → COMP8420-week3-solution.txt\n",
      "Converted: COMP8420-week4-solution.ipynb → COMP8420-week4-solution.txt\n",
      "Converted: COMP8420-week5-solution.ipynb → COMP8420-week5-solution.txt\n",
      "Converted: COMP8420-week2-solution.ipynb → COMP8420-week2-solution.txt\n",
      "Converted: COMP8420-week4-practice.ipynb → COMP8420-week4-practice.txt\n",
      "Converted: COMP8420-week7-solution.ipynb → COMP8420-week7-solution.txt\n",
      "Converted: COMP8420-week3-practice.ipynb → COMP8420-week3-practice.txt\n",
      "Converted: COMP8420-week6-solution.ipynb → COMP8420-week6-solution.txt\n",
      "Converted: COMP8420_week1_solution.ipynb → COMP8420_week1_solution.txt\n",
      "Converted: COMP8420_week12_solution.ipynb → COMP8420_week12_solution.txt\n",
      "Converted: COMP8420_week9_solution.ipynb → COMP8420_week9_solution.txt\n",
      "Converted: COMP8420_week8_solution.ipynb → COMP8420_week8_solution.txt\n",
      "Converted: COMP8420_week11_solution.ipynb → COMP8420_week11_solution.txt\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "# Define paths for notebooks and output .txt\n",
    "prac_folder = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Prac\")  # Folder with your .ipynb files\n",
    "output_folder = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/practicals\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to extract markdown + code\n",
    "def extract_notebook_text(nb_path):\n",
    "    nb = nbformat.read(open(nb_path, \"r\", encoding=\"utf-8\"), as_version=4)\n",
    "    content = []\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type in ['markdown', 'code']:\n",
    "            content.append(cell.source)\n",
    "    return \"\\n\\n\".join(content)\n",
    "\n",
    "# Convert all .ipynb files to .txt\n",
    "for nb_file in prac_folder.glob(\"*.ipynb\"):\n",
    "    text = extract_notebook_text(nb_file)\n",
    "    txt_name = nb_file.stem + \".txt\"\n",
    "    with open(output_folder / txt_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"Converted: {nb_file.name} → {txt_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c52f7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qna.json saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define Q&A dataset\n",
    "qa_data = [\n",
    "    {\n",
    "        \"question\": \"What is a foundation model in NLP?\",\n",
    "        \"answer\": \"A foundation model is a large pre-trained model trained on massive datasets, serving as the base for fine-tuning on specific NLP tasks.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are examples of foundation models?\",\n",
    "        \"answer\": \"Examples include GPT-3.5, Claude, PaLM, BERT, LLaMA, and Mistral.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does fine-tuning mean in the context of LLMs?\",\n",
    "        \"answer\": \"Fine-tuning is the process of continuing training on a pre-trained model using a smaller, task-specific dataset.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between prompt tuning and fine-tuning?\",\n",
    "        \"answer\": \"Prompt tuning adjusts input formatting without altering the model, while fine-tuning retrains the model on new data.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was covered in Week 6 of COMP8420?\",\n",
    "        \"answer\": \"Week 6 focused on fine-tuning large language models, covering parameter-efficient fine-tuning and adapter-based methods.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When is the COMP8420 project presentation due?\",\n",
    "        \"answer\": \"The presentation is scheduled for Week 13, Friday, June 6th, 2025.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the final deadline for Assignment 3?\",\n",
    "        \"answer\": \"Assignment 3 (code + report) is due during the exam period on June 17th, 2025.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What type of model should we use for our project?\",\n",
    "        \"answer\": \"You can use OpenAI’s GPT-3.5 or open-source models like LLaMA2 or Mistral, depending on your goals and data.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do we evaluate our NLP project?\",\n",
    "        \"answer\": \"Use metrics like BLEU, ROUGE, accuracy, and ablation studies to compare performance against baselines or alternatives.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are embedding models used for?\",\n",
    "        \"answer\": \"They convert text into vector form for similarity search and retrieval, commonly used in RAG pipelines.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do we retrieve answers in a RAG system?\",\n",
    "        \"answer\": \"Text chunks are embedded into vectors and searched via similarity to retrieve relevant content for generation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are lectures delivered in COMP8420?\",\n",
    "        \"answer\": \"Lectures are delivered via PDFs and practical notebooks, combining theory and hands-on exercises.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What technologies are used in this course?\",\n",
    "        \"answer\": \"Hugging Face, PyTorch, LangChain, OpenAI APIs, and vector databases like FAISS.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can we use ChatGPT in our project?\",\n",
    "        \"answer\": \"Yes, but your project must demonstrate additional engineering beyond just using the API.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is parameter-efficient fine-tuning?\",\n",
    "        \"answer\": \"It refers to fine-tuning techniques like LoRA and Adapters that update only a small subset of the model.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What should we cover in the presentation?\",\n",
    "        \"answer\": \"Your project title, real-world problem, methodology, expected outcome, and team contributions.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What’s the length of the presentation?\",\n",
    "        \"answer\": \"You must present for 3–4 minutes during the Week 13 Practice Workshop.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How can we improve our mark?\",\n",
    "        \"answer\": \"Make your project novel, apply evaluation methods, and clearly explain your work in the report and presentation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are some real-world NLP tasks?\",\n",
    "        \"answer\": \"Text classification, summarization, question answering, entity recognition, translation, and dialogue generation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can we build an agent using LangChain?\",\n",
    "        \"answer\": \"Yes, LangChain is commonly used to implement RAG-based agents using LLMs and vector stores.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save as qna.json\n",
    "with open(output_dir / \"qna.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_data, f, indent=2)\n",
    "\n",
    "print(\"qna.json saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "868fa154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deadlines.json saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Output directory (same as before)\n",
    "output_dir = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420\")\n",
    "\n",
    "# Define the deadlines\n",
    "deadlines = [\n",
    "    {\n",
    "        \"task\": \"Team Registration\",\n",
    "        \"due_date\": \"2025-05-30\",\n",
    "        \"type\": \"Workshop\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"Project Presentation (Week 13)\",\n",
    "        \"due_date\": \"2025-06-06\",\n",
    "        \"type\": \"Practice Workshop\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"Final Report + Code Submission\",\n",
    "        \"due_date\": \"2025-06-17\",\n",
    "        \"type\": \"Exam Period\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_dir / \"deadlines.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(deadlines, f, indent=2)\n",
    "\n",
    "print(\"deadlines.json saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc8be32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_info.txt saved successfully.\n"
     ]
    }
   ],
   "source": [
    "course_info_text = \"\"\"\n",
    "Course Title: COMP8420 – Advanced Natural Language Processing (S1 2025)\n",
    "\n",
    "Description:\n",
    "This course teaches students how to apply modern Natural Language Processing (NLP) techniques using large language models (LLMs). It focuses on real-world applications and responsible development practices. Topics include foundation models, prompt engineering, fine-tuning, RAG pipelines, privacy, security, and AI agent design.\n",
    "\n",
    "Teaching Team:\n",
    "- Dr. Qiongkai Xu (Lecturer)\n",
    "- Prof. Longbing Cao (Supervisor)\n",
    "- Mr. Weijun Li (TA)\n",
    "\n",
    "Key Technologies:\n",
    "- Hugging Face, PyTorch, OpenAI APIs, LangChain, FAISS\n",
    "\n",
    "Assessments:\n",
    "- Assignment 1: Text Classification\n",
    "- Assignment 2: Text Generation\n",
    "- Assignment 3: Team Project (Presentation + Code + Report)\n",
    "\n",
    "Objective:\n",
    "Prepare students to build and deploy intelligent NLP systems with ethical awareness and practical skills.\n",
    "\"\"\"\n",
    "\n",
    "# Save to .txt\n",
    "with open(output_dir / \"course_info.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(course_info_text.strip())\n",
    "\n",
    "print(\"course_info.txt saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aa5ab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "announcements.json saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420\")\n",
    "\n",
    "# Announcements dataset\n",
    "announcements = [\n",
    "    {\n",
    "        \"title\": \"Assignment 3 – Presentation Reminder\",\n",
    "        \"content\": \"Don't forget to prepare a 3–4 minute talk about your project for the Week 13 workshop (Friday June 6th).\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Assignment 3 Submission\",\n",
    "        \"content\": \"Final code and report must be submitted by June 17th during the exam period. Late submissions will not be accepted without special consideration.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Week 7 Workshop Topic\",\n",
    "        \"content\": \"We'll explore risks and safety issues in LLMs. Please review the Week 7 slides before the workshop.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Week 10 Team Registration\",\n",
    "        \"content\": \"Make sure to form your group and register your project title by the Week 10 workshop.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save as JSON\n",
    "with open(output_dir / \"announcements.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(announcements, f, indent=2)\n",
    "\n",
    "print(\"announcements.json saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4901839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discussions.json saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Sample discussion Q&A\n",
    "discussions = [\n",
    "    {\n",
    "        \"question\": \"Do we have to use LangChain for Assignment 3?\",\n",
    "        \"answer\": \"No, LangChain is optional. You can use any framework that supports RAG or LLM integration.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can we use ChatGPT API?\",\n",
    "        \"answer\": \"Yes, but remember that you must demonstrate your own engineering effort in addition to using the API.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is it okay to work solo on Assignment 3?\",\n",
    "        \"answer\": \"Projects should be completed in teams of two unless you’ve received special permission.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What’s the expected length of the presentation?\",\n",
    "        \"answer\": \"Each team should present for 3–4 minutes during the Week 13 practice workshop.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can we include public datasets in our project?\",\n",
    "        \"answer\": \"Yes, you may include public datasets as long as they’re relevant to your topic and cited properly.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save as JSON\n",
    "with open(output_dir / \"discussions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(discussions, f, indent=2)\n",
    "\n",
    "print(\"discussions.json saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f1277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Total chunks embedded: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/zw6f9z6j7tbf93ys9qbgldpw0000gn/T/ipykernel_11441/2309739295.py:63: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import nbformat\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, JSONLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Set your base dataset path\n",
    "dataset_path = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420\")\n",
    "persist_path = dataset_path / \"chroma_store\"\n",
    "persist_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "all_chunks = []\n",
    "\n",
    "# 1. Load Lecture PDFs\n",
    "lecture_path = dataset_path / \"lectures\"\n",
    "for pdf_file in lecture_path.glob(\"*.pdf\"):\n",
    "    loader = PyMuPDFLoader(str(pdf_file))\n",
    "    docs = loader.load()\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# 2. Load Practical Notebooks (.ipynb)\n",
    "pracs_path = dataset_path / \"practicals\"\n",
    "for ipynb_file in pracs_path.glob(\"*.ipynb\"):\n",
    "    nb = nbformat.read(open(ipynb_file, \"r\", encoding=\"utf-8\"), as_version=4)\n",
    "    text = \"\"\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type in [\"markdown\", \"code\"]:\n",
    "            text += cell.source + \"\\n\\n\"\n",
    "    doc = Document(page_content=text, metadata={\"source\": ipynb_file.name})\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# 3. Load JSON Files (qna, deadlines, etc.)\n",
    "json_files = [\"qna.json\", \"deadlines.json\", \"announcements.json\", \"discussions.json\"]\n",
    "for json_file in json_files:\n",
    "    loader = JSONLoader(\n",
    "        file_path=str(dataset_path / json_file),\n",
    "        jq_schema=\".[]\",\n",
    "        text_content=False\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# 4. Load course_info.txt\n",
    "info_loader = TextLoader(str(dataset_path / \"course_info.txt\"))\n",
    "docs = info_loader.load()\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "all_chunks.extend(chunks)\n",
    "\n",
    "# 5. Embed and save to Chroma\n",
    "embedding = OpenAIEmbeddings(disallowed_special=())\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_chunks,\n",
    "    embedding=embedding,\n",
    "    persist_directory=str(persist_path)\n",
    ")\n",
    "vectorstore.persist()\n",
    "\n",
    "print(f\"Done. Total chunks embedded: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a50ea",
   "metadata": {},
   "source": [
    "### Student QA Agent with Evaluation Metrics\n",
    "\n",
    "This section sets up the Student QA Agent using a Retrieval-Augmented Generation (RAG) pipeline with LangChain and GPT-3.5. The agent answers student queries by retrieving relevant course content from the vector store and generating responses.\n",
    "\n",
    "To assess the quality of the agent's answers, the following evaluation metrics are implemented:\n",
    "\n",
    "- **Precision@k**: Checks whether the correct source document (e.g., `qna.json`) is among the top retrieved documents.\n",
    "- **BLEU Score**: Measures textual overlap between the generated answer and a reference answer.\n",
    "- **Cosine Similarity**: Computes semantic similarity between the generated and reference answers using sentence embeddings.\n",
    "- **Response Time**: Tracks latency to measure system responsiveness.\n",
    "\n",
    "This helps quantify the performance of the agent and ensures it produces accurate, relevant, and helpful responses to students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9327dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/zw6f9z6j7tbf93ys9qbgldpw0000gn/T/ipykernel_35417/2142170615.py:14: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=persist_path, embedding_function=embedding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Assignment 3 (code + report) is due during the exam period on June 17th, 2025. Final code and report must be submitted by June 17th during the exam period. Late submissions will not be accepted without special consideration.\n",
      "\n",
      "Sources:\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/qna.json\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/announcements.json\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/discussions.json\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/qna.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d74b40a9612414a9a75970093d7a358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3689d5b016d045fdb8140c8995779b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e39b7dc93a744eb831ccff54272543b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a7684c0eae42ab8d5f4befeb46484d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdb724ccfa24a80ae97e6df450cf26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60aaea47d28f422aadd81bb0e2e0d222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b98e477ad6441da0b85e3bf7f2e843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f1f04c34044b8e804b9d97dd9661a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bf127079364b99baa124a4f7d09bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0fece48afa441f08eaeb38510cb7f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0427116c5f1e41df852b5f1bfd84606b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Precision@k       : 1\n",
      "BLEU Score        : 0.17\n",
      "Cosine Similarity : 0.83\n",
      "Response Time     : 1.69 sec\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your_Api_Key\"\n",
    "# === Setup ===\n",
    "embedding = OpenAIEmbeddings(disallowed_special=())\n",
    "persist_path = \"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/chroma_store\"\n",
    "vectorstore = Chroma(persist_directory=persist_path, embedding_function=embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# === Evaluation Utilities ===\n",
    "\n",
    "def precision_at_k(source_docs, expected_source):\n",
    "    for doc in source_docs:\n",
    "        if expected_source in doc.metadata.get(\"source\", \"\"):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def bleu_score_metric(reference_answer, generated_answer):\n",
    "    reference = [reference_answer.split()]\n",
    "    candidate = generated_answer.split()\n",
    "    return sentence_bleu(reference, candidate)\n",
    "\n",
    "def cosine_similarity_metric(reference_answer, generated_answer):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    emb_ref = model.encode(reference_answer, convert_to_tensor=True)\n",
    "    emb_gen = model.encode(generated_answer, convert_to_tensor=True)\n",
    "    return util.cos_sim(emb_gen, emb_ref).item()\n",
    "\n",
    "# === Query & Evaluation ===\n",
    "\n",
    "query = \"When is Assignment 3 due and how do we submit it?\"\n",
    "reference_answer = \"Assignment 3 is due on June 17th, 2025. Submit your code and report during the exam period via iLearn.\"\n",
    "expected_source = \"qna.json\"\n",
    "\n",
    "start_time = time.time()\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "end_time = time.time()\n",
    "\n",
    "# Output answer\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])\n",
    "\n",
    "# Output sources\n",
    "print(\"\\nSources:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata.get(\"source\", \"Unknown\"))\n",
    "\n",
    "# Evaluation\n",
    "prec = precision_at_k(response[\"source_documents\"], expected_source)\n",
    "bleu = bleu_score_metric(reference_answer, response[\"result\"])\n",
    "cos_sim = cosine_similarity_metric(reference_answer, response[\"result\"])\n",
    "latency = end_time - start_time\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Precision@k       : {prec}\")\n",
    "print(f\"BLEU Score        : {bleu:.2f}\")\n",
    "print(f\"Cosine Similarity : {cos_sim:.2f}\")\n",
    "print(f\"Response Time     : {latency:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab710758",
   "metadata": {},
   "source": [
    "### Teacher Agent – Quiz Generator with Evaluation\n",
    "\n",
    "This section implements an intelligent quiz generator that creates multiple-choice questions (MCQs) from lecture content using OpenAI's GPT-3.5 model. The generator extracts key concepts from lecture `.txt` files and formulates 3–5 quiz questions per file, making it easier for teachers to assess student understanding.\n",
    "\n",
    "In addition to generating quizzes, the system also performs **automatic structural evaluation** on the output to ensure quality and consistency. This includes checks for completeness, proper answer formatting, and option uniqueness.\n",
    "\n",
    "**Inputs:**\n",
    "- `.txt` file containing the lecture content\n",
    "- Number of questions to generate (default is 5)\n",
    "\n",
    "**Outputs:**\n",
    "- A list of MCQs with four options (A–D) and a correct answer\n",
    "- Optional raw and structured quiz files (`.txt` and `.json`)\n",
    "- Evaluation metrics:\n",
    "  - % of complete questions\n",
    "  - % with valid answer format (A–D)\n",
    "  - % with unique answer options\n",
    "\n",
    "This tool is especially useful for teachers to **automate assessment** and ensure quiz quality before sharing with students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa49fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def generate_quiz_from_lecture(\n",
    "    lecture_path,\n",
    "    n_questions=5,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    save_txt_path=None,\n",
    "    save_json_path=None,\n",
    "    truncate_tokens=4000\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate MCQs from lecture content using GPT-3.5 via LangChain and evaluate their structure.\n",
    "    \"\"\"\n",
    "    lecture_path = Path(lecture_path)\n",
    "\n",
    "    # === Read and truncate content ===\n",
    "    with open(lecture_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    if len(content) > truncate_tokens:\n",
    "        content = content[:truncate_tokens] + \"\\n\\n[Content truncated]\"\n",
    "\n",
    "    # === Prompt GPT ===\n",
    "    prompt = f\"\"\"\n",
    "You are a quiz-generating assistant. Based on the following lecture content, generate {n_questions} multiple-choice questions (MCQs). Each question should have:\n",
    "\n",
    "- 1 clear question\n",
    "- 4 options labeled A, B, C, and D\n",
    "- The correct answer at the end, formatted like: Answer: B\n",
    "\n",
    "Lecture content:\n",
    "\\\"\\\"\\\"\n",
    "{content}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "    chat = ChatOpenAI(model_name=model, temperature=0)\n",
    "    raw_output = chat.predict(prompt)\n",
    "\n",
    "    # === Save raw text ===\n",
    "    if save_txt_path:\n",
    "        with open(save_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(raw_output)\n",
    "        print(f\"Saved raw quiz to: {save_txt_path}\")\n",
    "\n",
    "    # === Parse to JSON ===\n",
    "    parsed_questions = []\n",
    "    current_q = {}\n",
    "    lines = raw_output.strip().splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith(\"Q\") or line[0].isdigit():\n",
    "            if current_q:\n",
    "                parsed_questions.append(current_q)\n",
    "                current_q = {}\n",
    "            current_q[\"question\"] = line.split(\":\", 1)[-1].strip()\n",
    "            current_q[\"options\"] = {}\n",
    "        elif line[0] in [\"A\", \"B\", \"C\", \"D\"] and line[1] == \".\":\n",
    "            key = line[0]\n",
    "            current_q[\"options\"][key] = line[2:].strip()\n",
    "        elif line.lower().startswith(\"answer:\"):\n",
    "            current_q[\"answer\"] = line.split(\":\")[-1].strip().upper()\n",
    "    if current_q:\n",
    "        parsed_questions.append(current_q)\n",
    "\n",
    "    # === Save JSON ===\n",
    "    if save_json_path:\n",
    "        with open(save_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(parsed_questions, f, indent=2)\n",
    "        print(f\"Saved parsed quiz to: {save_json_path}\")\n",
    "\n",
    "    # === Evaluation ===\n",
    "    def evaluate_questions(questions):\n",
    "        total = len(questions)\n",
    "        complete = 0\n",
    "        valid_answer = 0\n",
    "        unique_options = 0\n",
    "\n",
    "        for q in questions:\n",
    "            # Check completeness\n",
    "            if \"question\" in q and \"options\" in q and len(q[\"options\"]) == 4 and \"answer\" in q:\n",
    "                complete += 1\n",
    "            # Check answer is A/B/C/D\n",
    "            if q.get(\"answer\") in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                valid_answer += 1\n",
    "            # Check all options are unique\n",
    "            opts = list(q.get(\"options\", {}).values())\n",
    "            if len(opts) == len(set(opts)):\n",
    "                unique_options += 1\n",
    "\n",
    "        return {\n",
    "            \"Total Questions\": total,\n",
    "            \"Complete Questions\": complete,\n",
    "            \"Valid Answer Format\": valid_answer,\n",
    "            \"Unique Option Sets\": unique_options,\n",
    "            \"Completeness (%)\": round(100 * complete / total, 1) if total else 0,\n",
    "            \"Answer Validity (%)\": round(100 * valid_answer / total, 1) if total else 0,\n",
    "            \"Option Uniqueness (%)\": round(100 * unique_options / total, 1) if total else 0,\n",
    "        }\n",
    "\n",
    "    metrics = evaluate_questions(parsed_questions)\n",
    "\n",
    "    # === Print metrics ===\n",
    "    print(\"\\nEvaluation Metrics for Generated Quiz:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key:25}: {value}\")\n",
    "\n",
    "    return parsed_questions, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14117e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/zw6f9z6j7tbf93ys9qbgldpw0000gn/T/ipykernel_35417/3097080918.py:37: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(model_name=model, temperature=0)\n",
      "/var/folders/n9/zw6f9z6j7tbf93ys9qbgldpw0000gn/T/ipykernel_35417/3097080918.py:38: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  raw_output = chat.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw quiz to: week6_quiz_raw.txt\n",
      "Saved parsed quiz to: week6_quiz.json\n",
      "\n",
      "Evaluation Metrics for Generated Quiz:\n",
      "Total Questions          : 5\n",
      "Complete Questions       : 5\n",
      "Valid Answer Format      : 5\n",
      "Unique Option Sets       : 5\n",
      "Completeness (%)         : 100.0\n",
      "Answer Validity (%)      : 100.0\n",
      "Option Uniqueness (%)    : 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'question': '1. What is the focus of the lecture on developing LLMs?',\n",
       "   'options': {'A': 'Data visualization techniques',\n",
       "    'B': 'Training and fine-tuning LLMs',\n",
       "    'C': 'Software development best practices',\n",
       "    'D': 'Hardware optimization strategies'},\n",
       "   'answer': 'B'},\n",
       "  {'question': \"2. Which technique is NOT mentioned in the agenda for this week's lecture?\",\n",
       "   'options': {'A': 'Prompt engineering',\n",
       "    'B': 'Chain of Thoughts',\n",
       "    'C': 'Image recognition algorithms',\n",
       "    'D': 'Retrieval-augmented generation'},\n",
       "   'answer': 'C'},\n",
       "  {'question': '3. What is emphasized in prompt engineering for Large Language Models?',\n",
       "   'options': {'A': 'Complex and ambiguous prompts',\n",
       "    'B': 'Instructions without any description',\n",
       "    'C': 'Vague and unclear prompts',\n",
       "    'D': 'Clear and precise prompts'},\n",
       "   'answer': 'D'},\n",
       "  {'question': '4. Which of the following is NOT a component of prompt engineering discussed in the lecture?',\n",
       "   'options': {'A': 'Role prompt',\n",
       "    'B': 'One-shot prompt',\n",
       "    'C': 'Chain of thought',\n",
       "    'D': 'Tree of thoughts'},\n",
       "   'answer': 'B'},\n",
       "  {'question': '5. Where can students find a comprehensive review on the potential of prompt engineering in Large Language Models?',\n",
       "   'options': {'A': 'University library',\n",
       "    'B': 'Lecture slides',\n",
       "    'C': 'Online resources/public domains',\n",
       "    'D': 'Personal notes'},\n",
       "   'answer': 'C'}],\n",
       " {'Total Questions': 5,\n",
       "  'Complete Questions': 5,\n",
       "  'Valid Answer Format': 5,\n",
       "  'Unique Option Sets': 5,\n",
       "  'Completeness (%)': 100.0,\n",
       "  'Answer Validity (%)': 100.0,\n",
       "  'Option Uniqueness (%)': 100.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_quiz_from_lecture(\n",
    "    lecture_path=\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/lectures/COMP8420-W6 - Dev LLMs - Fine-tuning.txt\",\n",
    "    n_questions=5,\n",
    "    save_txt_path=\"week6_quiz_raw.txt\",\n",
    "    save_json_path=\"week6_quiz.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bf030",
   "metadata": {},
   "source": [
    "### Student Agent – Generate Demo Quiz from Topic\n",
    "\n",
    "This feature allows students to instantly generate a short quiz on any course-related topic (e.g., *\"fine-tuning\"*, *\"transformers\"*, etc.). It helps reinforce understanding through self-assessment.\n",
    "\n",
    "**How It Works:**\n",
    "- Takes a student query (topic or concept)\n",
    "- Retrieves the most relevant course materials using **vector similarity search** (RAG)\n",
    "- Uses **GPT-3.5** to generate **2–3 multiple-choice questions (MCQs)**\n",
    "- Returns each question with:\n",
    "  - A clear question statement\n",
    "  - Four answer options (A–D)\n",
    "  - One correct answer\n",
    "\n",
    "**Quiz Quality Evaluation:**\n",
    "After generating the quiz, the system automatically evaluates the structural integrity of the output:\n",
    "- Are all questions complete?\n",
    "- Do all questions have 4 unique options?\n",
    "- Is the answer format valid (A/B/C/D)?\n",
    "- Are the questions properly formatted?\n",
    "\n",
    "This ensures quiz quality before students rely on it for revision or learning.\n",
    "\n",
    "**Educational Value:**\n",
    "This self-service agent enables personalized quiz creation for active learning, and supports **automated content reinforcement** — especially useful before exams or tutorials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9727adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "def generate_demo_quiz_from_query(query, vectorstore_path, model=\"gpt-3.5-turbo\", n_questions=3):\n",
    "    \"\"\"\n",
    "    Generate a short quiz based on a student's topic query using retrieved course content and GPT-3.5.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): Topic or question from the student\n",
    "    - vectorstore_path (str): Path to Chroma vector database\n",
    "    - model (str): OpenAI model name\n",
    "    - n_questions (int): Number of quiz questions to generate\n",
    "\n",
    "    Returns:\n",
    "    - str: Generated quiz as text\n",
    "    \"\"\"\n",
    "    # Load vector DB and embedding\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma(persist_directory=vectorstore_path, embedding_function=embedding)\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])[:3500]\n",
    "\n",
    "    # Prompt for quiz generation\n",
    "    prompt = f\"\"\"\n",
    "You are an AI teaching assistant. Based on the following course content, generate {n_questions} multiple-choice questions (MCQs) to test understanding. Each question should include:\n",
    "- A clear question\n",
    "- 4 answer options (A, B, C, D)\n",
    "- The correct answer clearly labeled at the end (e.g., Answer: C)\n",
    "\n",
    "Course content:\n",
    "\\\"\\\"\\\"\n",
    "{context}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "    # Run GPT\n",
    "    llm = ChatOpenAI(model_name=model, temperature=0)\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Extract and print quiz\n",
    "    quiz_text = response.content.strip() if hasattr(response, \"content\") else str(response)\n",
    "    print(\"Quiz Based on Your Query:\\n\")\n",
    "    print(textwrap.indent(quiz_text, \"  \"))\n",
    "\n",
    "    return quiz_text\n",
    "\n",
    "\n",
    "def evaluate_generated_quiz(quiz_text):\n",
    "    \"\"\"\n",
    "    Basic structural evaluation of a quiz generated from text.\n",
    "\n",
    "    Returns a dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    lines = quiz_text.strip().splitlines()\n",
    "    questions = []\n",
    "    current_q = {\"options\": {}, \"answer\": None}\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if re.match(r\"^\\d+\\.\", line) or line.lower().startswith(\"q\"):\n",
    "            if current_q[\"options\"] or current_q[\"answer\"]:\n",
    "                questions.append(current_q)\n",
    "                current_q = {\"options\": {}, \"answer\": None}\n",
    "            current_q[\"question\"] = line\n",
    "        elif re.match(r\"^[A-D]\\)\", line):\n",
    "            key = line[0]\n",
    "            current_q[\"options\"][key] = line[2:].strip()\n",
    "        elif \"answer\" in line.lower():\n",
    "            answer_match = re.search(r\"([A-D])\", line.upper())\n",
    "            if answer_match:\n",
    "                current_q[\"answer\"] = answer_match.group(1)\n",
    "\n",
    "    if current_q.get(\"options\") or current_q.get(\"answer\"):\n",
    "        questions.append(current_q)\n",
    "\n",
    "    # Evaluate\n",
    "    total = len(questions)\n",
    "    complete = sum(1 for q in questions if len(q[\"options\"]) == 4 and q[\"answer\"] in [\"A\", \"B\", \"C\", \"D\"])\n",
    "    unique_options = sum(1 for q in questions if len(set(q[\"options\"].values())) == 4)\n",
    "\n",
    "    results = {\n",
    "        \"Total Questions\": total,\n",
    "        \"Complete Questions\": complete,\n",
    "        \"Valid Answer Format\": complete,\n",
    "        \"Unique Option Sets\": unique_options,\n",
    "        \"Completeness (%)\": round(complete / total * 100, 2) if total else 0,\n",
    "        \"Answer Validity (%)\": round(complete / total * 100, 2) if total else 0,\n",
    "        \"Option Uniqueness (%)\": round(unique_options / total * 100, 2) if total else 0\n",
    "    }\n",
    "\n",
    "    print(\"\\nEvaluation of Generated Quiz:\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k:25}: {v}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b724e0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/zw6f9z6j7tbf93ys9qbgldpw0000gn/T/ipykernel_35417/3667882390.py:25: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quiz Based on Your Query:\n",
      "\n",
      "  1. What does fine-tuning mean in the context of LLMs?\n",
      "  A) Fine-tuning is the process of training a model from scratch\n",
      "  B) Fine-tuning is the process of training a model on a large, general dataset\n",
      "  C) Fine-tuning is the process of continuing training on a pre-trained model using a smaller, task-specific dataset\n",
      "  D) Fine-tuning is the process of freezing all model parameters\n",
      "\n",
      "  Answer: C\n",
      "\n",
      "  2. What is parameter-efficient fine-tuning?\n",
      "  A) It refers to training a model with a large number of parameters\n",
      "  B) It refers to fine-tuning a model on a large, general dataset\n",
      "  C) It refers to fine-tuning techniques like LoRA and Adapters that update only a small subset of the model\n",
      "  D) It refers to fine-tuning a model without updating any parameters\n",
      "\n",
      "  Answer: C\n",
      "\n",
      "  3. Which fine-tuning technique updates only a small subset of the model?\n",
      "  A) Gradient Descent\n",
      "  B) LoRA\n",
      "  C) Random Forest\n",
      "  D) Support Vector Machines\n",
      "\n",
      "  Answer: B\n",
      "\n",
      "Evaluation of Generated Quiz:\n",
      "Total Questions          : 3\n",
      "Complete Questions       : 3\n",
      "Valid Answer Format      : 3\n",
      "Unique Option Sets       : 3\n",
      "Completeness (%)         : 100.0\n",
      "Answer Validity (%)      : 100.0\n",
      "Option Uniqueness (%)    : 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Total Questions': 3,\n",
       " 'Complete Questions': 3,\n",
       " 'Valid Answer Format': 3,\n",
       " 'Unique Option Sets': 3,\n",
       " 'Completeness (%)': 100.0,\n",
       " 'Answer Validity (%)': 100.0,\n",
       " 'Option Uniqueness (%)': 100.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz = generate_demo_quiz_from_query(\n",
    "    query=\"fine-tuning\",\n",
    "    vectorstore_path=\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/chroma_store\"\n",
    ")\n",
    "\n",
    "evaluate_generated_quiz(quiz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7292406",
   "metadata": {},
   "source": [
    "### Admin Agent – LMS Statistics Dashboard\n",
    "\n",
    "This section provides a summary of LMS activity across different sources:\n",
    "- `qna.json` – student Q&A data\n",
    "- `announcements.json` – instructor/admin posts\n",
    "- `discussions.json` – forum or general discussion threads\n",
    "\n",
    "The dashboard gives:\n",
    "- Total number of entries per source\n",
    "- Frequency of key academic terms (e.g., “assignment”, “quiz”, “exam”)\n",
    "- Basic engagement insights\n",
    "\n",
    "This helps administrative users understand where student attention is focused and identify recurring topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3878b28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMS Statistics Dashboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Total Entries</th>\n",
       "      <th>assignment</th>\n",
       "      <th>quiz</th>\n",
       "      <th>exam</th>\n",
       "      <th>deadline</th>\n",
       "      <th>submission</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qna.json</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>announcements.json</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>discussions.json</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 File  Total Entries  assignment  quiz  exam  deadline  \\\n",
       "0            qna.json             20           2     0     3         1   \n",
       "1  announcements.json              4           2     0     1         0   \n",
       "2    discussions.json              5           2     0     0         0   \n",
       "\n",
       "   submission  feedback  \n",
       "0           0         0  \n",
       "1           2         0  \n",
       "2           0         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Keyword Mentions Across All Sources:\n",
      "Assignment  : 6\n",
      "Quiz        : 0\n",
      "Exam        : 4\n",
      "Deadline    : 1\n",
      "Submission  : 2\n",
      "Feedback    : 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "# === Configuration ===\n",
    "data_dir = Path(\"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/\")\n",
    "filenames = [\"qna.json\", \"announcements.json\", \"discussions.json\"]\n",
    "keywords = [\"assignment\", \"quiz\", \"exam\", \"deadline\", \"submission\", \"feedback\"]\n",
    "\n",
    "# === Helper Functions ===\n",
    "def load_json_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def count_keywords(text, keywords):\n",
    "    text = text.lower()\n",
    "    return {kw: text.count(kw) for kw in keywords}\n",
    "\n",
    "# === Processing ===\n",
    "summary = []\n",
    "keyword_totals = defaultdict(int)\n",
    "\n",
    "for fname in filenames:\n",
    "    path = data_dir / fname\n",
    "    data = load_json_file(path)\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        continue\n",
    "\n",
    "    total_entries = len(data)\n",
    "    all_text = \" \".join(json.dumps(item).lower() for item in data)\n",
    "    keyword_counts = count_keywords(all_text, keywords)\n",
    "    \n",
    "    for kw, count in keyword_counts.items():\n",
    "        keyword_totals[kw] += count\n",
    "    \n",
    "    summary.append({\n",
    "        \"File\": fname,\n",
    "        \"Total Entries\": total_entries,\n",
    "        **keyword_counts\n",
    "    })\n",
    "\n",
    "# === Display Results ===\n",
    "df_summary = pd.DataFrame(summary)\n",
    "print(\"LMS Statistics Dashboard\")\n",
    "display(df_summary)\n",
    "\n",
    "# === Keyword Total Summary (across all files) ===\n",
    "print(\"\\nTotal Keyword Mentions Across All Sources:\")\n",
    "for kw in keywords:\n",
    "    print(f\"{kw.title():<12}: {keyword_totals[kw]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4c09ae",
   "metadata": {},
   "source": [
    "### Admin Agent – Natural Language Query & Summary\n",
    "\n",
    "This component allows administrators or instructors to ask natural language questions about the course data (e.g., Q&A posts, announcements, discussions). It uses Retrieval-Augmented Generation (RAG) to fetch the most relevant content from embedded LMS sources and generates summaries using GPT-3.5.\n",
    "\n",
    "**Key Features:**\n",
    "- Enables free-form queries like:\n",
    "  - \"What are students asking about Assignment 3?\"\n",
    "  - \"Summarize concerns related to exams\"\n",
    "  - \"Are there any posts discussing submission issues?\"\n",
    "- Uses vector similarity search to retrieve top-matching documents\n",
    "- Summarizes retrieved content with a large language model (LLM)\n",
    "\n",
    "**How It Works:**\n",
    "- Embedded LMS content is stored in a Chroma vector database\n",
    "- Queries are matched to the most relevant content using vector similarity\n",
    "- GPT-3.5 generates a concise response based on retrieved information\n",
    "\n",
    "This makes it easy for instructors to understand student concerns, spot common topics, and make data-informed decisions without reading every post manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10017f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      "Students are asking about the final deadline for Assignment 3, whether it is okay to work solo on the assignment, if LangChain is required for Assignment 3, and details about the submission requirements for Assignment 3.\n",
      "\n",
      "Source Files:\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/discussions.json\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/qna.json\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/announcements.json\n",
      "\n",
      "\n",
      "Answer (Query: Exam Concerns):\n",
      "\n",
      "I don't know.\n",
      "\n",
      "Source Files:\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/qna.json\n",
      "- /Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/announcements.json\n",
      "\n",
      "Evaluation Metrics (Second Query):\n",
      "Response Time (sec)     : 1.32\n",
      "Unique Source Files     : 2\n",
      "Total Chunks Retrieved  : 4\n",
      "\n",
      "Evaluation Metrics:\n",
      "Response Time (sec)     : 1.98\n",
      "Unique Source Files     : 3\n",
      "Total Chunks Retrieved  : 4\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import time\n",
    "\n",
    "# === Configuration ===\n",
    "vectorstore_path = \"/Users/shaimonrahman/Desktop/COMP8420/Assignment_3/StudentAgentDataset/COMP8420/chroma_store\"\n",
    "embedding = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Load vector store & retriever\n",
    "vectorstore = Chroma(persist_directory=vectorstore_path, embedding_function=embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# Setup QA Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# === Query and Evaluation ===\n",
    "query = \"What are students asking about Assignment 3?\"\n",
    "start_time = time.time()\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "end_time = time.time()\n",
    "\n",
    "# Extract result and sources\n",
    "result = response[\"result\"]\n",
    "sources = [doc.metadata.get(\"source\", \"Unknown\") for doc in response[\"source_documents\"]]\n",
    "unique_sources = set(sources)\n",
    "\n",
    "# Print answer\n",
    "print(\"Answer:\\n\")\n",
    "print(result)\n",
    "\n",
    "# Print sources\n",
    "print(\"\\nSource Files:\")\n",
    "for src in unique_sources:\n",
    "    print(\"-\", src)\n",
    "\n",
    "# === Second Admin Query ===\n",
    "query2 = \"Are there any common concerns about the exam?\"\n",
    "start_time2 = time.time()\n",
    "response2 = qa_chain.invoke({\"query\": query2})\n",
    "end_time2 = time.time()\n",
    "\n",
    "result2 = response2[\"result\"]\n",
    "sources2 = [doc.metadata.get(\"source\", \"Unknown\") for doc in response2[\"source_documents\"]]\n",
    "unique_sources2 = set(sources2)\n",
    "\n",
    "# Print second answer\n",
    "print(\"\\n\\nAnswer (Query: Exam Concerns):\\n\")\n",
    "print(result2)\n",
    "\n",
    "# Print second sources\n",
    "print(\"\\nSource Files:\")\n",
    "for src in unique_sources2:\n",
    "    print(\"-\", src)\n",
    "\n",
    "# Second evaluation metrics\n",
    "latency2 = round(end_time2 - start_time2, 2)\n",
    "source_count2 = len(unique_sources2)\n",
    "\n",
    "print(\"\\nEvaluation Metrics (Second Query):\")\n",
    "print(f\"Response Time (sec)     : {latency2}\")\n",
    "print(f\"Unique Source Files     : {source_count2}\")\n",
    "print(f\"Total Chunks Retrieved  : {len(response2['source_documents'])}\")\n",
    "\n",
    "\n",
    "# Evaluation Metrics\n",
    "latency = round(end_time - start_time, 2)\n",
    "source_count = len(unique_sources)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Response Time (sec)     : {latency}\")\n",
    "print(f\"Unique Source Files     : {source_count}\")\n",
    "print(f\"Total Chunks Retrieved  : {len(response['source_documents'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da341f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
