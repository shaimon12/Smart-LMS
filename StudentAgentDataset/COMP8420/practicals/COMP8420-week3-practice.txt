# COMP8420 Advanced Natural Language Processing
## Week 3 - Transformer Models

## Today's Topics
- Transformer Model Architectures
- Tokenization
- Embedding
- Self-Attention Mechanism


# We will use below visulization tool and let's install it in advance.
!pip install bertviz

### 1. Transformer Model Architectures

The Transformer is a deep learning architecture developed by researchers at Google, based on the **multi-head self-attention mechanism** proposed in the 2017 paper "[Attention Is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)".  

The figure below presents the architecture of the proposed Transformer model, which consists of both **encoder** and **decoder** components and is designed for text translation tasks (e.g., English-to-German).  

Figure source: https://jalammar.github.io/illustrated-transformer/

<center>
    <img src="https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/assets/transformers.png" width="90%">
</center>

Modern NLP models fall into three main categories, each suited for different tasks:

- **Encoder-Only** Models (e.g., Google BERT - [Model](https://huggingface.co/google-bert/bert-base-uncased), [Paper](https://aclanthology.org/N19-1423/))
  - Typical tasks: Classification, sentiment analysis, masked language model
  - Mechanism: $P(Y|X_1,...,X_n)$
  - Examples:
    - Classify "I like NLP" as positive sentiment
    - Predict "like" in "I [MASK] artificial intelligence."

- **Decoder-Only** Models (e.g., OpenAI GPT-2 - [Model](https://huggingface.co/openai-community/gpt2), [Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf))
  - Typical tasks: Text generation, chatbots  
  - Mechanism: $P(X_n|X_1,...,X_{n-1})$
  - Example: Given "I like artificial", predict the probability of "intelligence"

- **Encoder-Decoder** Models (e.g., Google T5 - [Model](https://huggingface.co/google-t5/t5-base), [Paper](https://arxiv.org/pdf/1910.10683))
  - Typical tasks: Translation, summarization
  - Mechanism:
    - Encoder: $P(Z|X_1,...,X_n)$, where Z is the intermediate representation
    - Decoder: $P(Y_i|Y_1,...,Y_{i-1},Z)$
  - Example: French "Je suis Ã©tudiant" â†’ English "I am a student"

- **Quick Comparison**

\begin{array}{|c|c|c|}
\hline
\textbf{Model Type} & \textbf{Best For} & \textbf{Example} \\
\hline
\text{Decoder} & \text{Generating text} & \text{Chatbots, writing assistants} \\
\hline
\text{Encoder} & \text{Understanding text} & \text{Sentiment analysis, classification} \\
\hline
\text{Encoder-Decoder} & \text{Transforming text} & \text{Translation, summarization} \\
\hline
\end{array}

ðŸ’¡ **Note**: The choice of architecture is not strictly tied to specific tasks.  

For example, classification and translation can be framed as generative tasksâ€”classification involves generating a label, while translation involves generating a new text.  

This flexibility is one reason many modern models increasingly adopt a decoder-only architecture.  

Let's load some models and examine their architectural differences.

#### **Encoder-Only Models**  

First, let's load an encoder model using [*bert-base-uncased*](https://huggingface.co/google-bert/bert-base-uncased) as an example.

# Load an encoder model
from transformers import AutoModelForSequenceClassification

bert_model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")

# Print model parameter names
for name, param in bert_model.named_parameters():
    print(name)               # Shows the parameter name
    # print(name, param.shape)  # Shows the name, and also shape of each parameter

#### **Decoder-Only Models**  

Next, let's practice loading a decoder model.  

**Task:**  
- Load [*GPT-2*](https://huggingface.co/openai-community/gpt2).  
- Print its parameters and examine its structure:  
  - How many Transformer layers does GPT-2 have?  
  - What modules does each layer consist of?  

# Q1: Please load the GPT2 model.


# Q2: Please print the parameter list of GPT2 model.


#### **Encoder-Decoder Models**  

Let's also load an encoder-decoder model, such as [*T5-Base*](https://huggingface.co/google-t5/t5-base).  

**Task:**  
- Load [*T5-Base*](https://huggingface.co/google-t5/t5-base).  
- Print its parameters and examine its structure:  
  - How many Transformer layers does T5-Base have in its encoder and decoder?   
  - What modules does each encoder and decoder layer consist of?  

# Q3: Please load the T5-base model.


# Q4: Please print the parameter list of T5-base model.


### 2. Tokenization

Despite their architectural differences, all these models share the same initial step in text processing:  
- **Transforming discrete text into numerical form.**  

This process can be divided into two steps:  
- **Tokenization**: The text is first broken down into discrete units called **tokens**, each mapped to a unique **numerical index**.  
- **Embedding Lookup**: These indices are then converted into **embedding vectors** by retrieving them from a **word embedding table**.  

Next, we will focus on the first stepâ€”**tokenization**â€”to understand 1) how a vocabulary is built and 2) how text is transformed into tokens.

#### 2.1 Building a Vocabulary from Corpus (Byte-Pair Encoding Algorithm ([BPE](https://huggingface.co/learn/nlp-course/en/chapter6/5)) as an example)

BPE builds a vocabulary for tokenization in two simple steps:

1. **Initialize**: Split all text into individual characters to create a base vocabulary.

2. **Merge**: Iteratively find and merge the most frequent pair of adjacent tokens, adding each new merged token to the vocabulary.

Example with "low lower lowest":

  - Step 1: Initialize
    - Text: "low lower lowest"
    - Split into characters: "l o w _ l o w e r _ l o w e s t" (where _ represents space)
    - Initial vocabulary: ["l", "o", "w", "_", "e", "r", "s", "t"]

  - Step 2: Merge Highest Frequency Subwords Iteratively
    - **Merge 1**: "l o" appears 3 times â†’ merge into "lo"
      - Text becomes: "lo w _ lo w e r _ lo w e s t"
      - Updated vocabulary: ["l", "o", "w", "_", "e", "r", "s", "t", "lo"]

    - **Merge 2**: "lo w" appears 3 times â†’ merge into "low"
      - Text becomes: "low _ low e r _ low e s t"
      - Updated vocabulary: ["l", "o", "w", "_", "e", "r", "s", "t", "lo", "low"]

      ...

    - Continue merging frequent pairs until desired vocabulary size is reached.

### Modified based on HuggingFace tokenizer documentation: https://huggingface.co/docs/tokenizers/python/v0.10.0/tutorials/python/training_from_memory.html

from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import ByteLevel

tokenizer = Tokenizer(BPE())
tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)

### Copy following text from HuggingFace: https://huggingface.co/learn/nlp-course/en/chapter6/5
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]

trainer = BpeTrainer(
    special_tokens=["<|endoftext|>"],
    vocab_size=50,
    min_frequency=2,
    show_progress=True
)

tokenizer.train_from_iterator(corpus, trainer)

vocab = tokenizer.get_vocab()   # Retrieve the vocabulary as a dictionary {token: id}.
sorted_vocab = dict(sorted(vocab.items(), key=lambda x: x[1]))   # Sort the vocabulary by token ID in ascending order.
print("Vocabulary:", list(sorted_vocab.keys()))
print("Token IDs:", list(sorted_vocab.values()))

text = "This is a tokenization test."
encoded = tokenizer.encode(text)
print("\nTokenized Output:", encoded.tokens)
print("Token IDs:", encoded.ids)

Let's load *bert-base-uncased* tokenizer and examine its vocabulary.

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")

## Q5: How can we obtain the vocabulary dictionary?
vocab = ...

print("Vocabulary Size:", len(vocab))
print("Sample tokens:", list(vocab.keys())[:10])

ðŸ’¡ **Practice:**  

We can also inspect the complete vocabulary from a model's source files. Do you know where and how?  

Hint: Try finding the vocabulary file for **bert-base-uncased** on its [Hugging Face page](https://huggingface.co/google-bert/bert-base-uncased).  

#### 2.2 Mapping input text into tokens

After building the vocabulary, we can tokenize any words, by finding the longest matching token in the vocabulary and get its ID.

sentence = "Macquarie University is an Australian university located in Sydney NSW."

tokens = tokenizer.tokenize(sentence)
print(f"\nTokens: {tokens}")

token_ids = tokenizer.encode(sentence)
print(f"\nToken IDs: {token_ids}")

decoded_text = tokenizer.decode(token_ids)
print(f"\nDecoded text: {decoded_text}")

Different models can have different vocabularies, leading to variations in tokenization results.  

Try loading the **[GPT-2](https://huggingface.co/openai-community/gpt2) tokenizer** and explore the following:  

1. How many tokens are in the GPT-2 vocabulary?  
2. How does GPT-2 tokenize the same sentence?

## Q6: Please try to load GPT-2 tokenizer and practice.

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

## Print the vocabulary size.
vocab = ...
print("Vocabulary Size:", len(vocab))

## Print the tokenization results on given sentence.
tokens = ...
print(f"\nTokens: {tokens}")

token_ids = ...
print(f"\nToken IDs: {token_ids}")

## Print the decoded sentence from token ids.
decoded_text = ...
print(f"\nDecoded text: {decoded_text}")

### 3. Embedding Vectors

By far, we have explored how to map the text to tokens and input IDs, but still they are discrete representations, which cannot be directly processed or learned by neural networks.

We need to map these discrete tokens into continuous embedding vectors, which is essentially a table lookup operation using token IDs.

#### Embedding layers

from transformers import AutoModelForSequenceClassification

bert_model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")

# Print model parameter names
for name, param in bert_model.named_parameters():
    if "embeddings" in name:
      print(name, param.shape)  # Shows the name, and also shape of each parameter

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

sentence1 = "Macquarie University is an Australian university located in Sydney NSW."
sentence2 = "It has a beautiful campus."

### Tokenization results for a single sentence.
encoding = tokenizer(sentence1, return_tensors="pt", return_token_type_ids=True)
input_ids = encoding["input_ids"]
token_type_ids = encoding["token_type_ids"]
position_ids = encoding["input_ids"].new_tensor(range(input_ids.shape[1])).unsqueeze(0)
print("Input IDs:", input_ids)
print("Position IDs:", position_ids)
print("Token Type IDs:", token_type_ids, "\n")

### Tokenization results for a pair of sentences.
encoding = tokenizer(sentence1, sentence2, return_tensors="pt", return_token_type_ids=True)
input_ids = encoding["input_ids"]
token_type_ids = encoding["token_type_ids"]
position_ids = encoding["input_ids"].new_tensor(range(input_ids.shape[1])).unsqueeze(0)
print("Input IDs:", input_ids)
print("Position IDs:", position_ids)
print("Token Type IDs:", token_type_ids)

In a BERT model, for each token, its final embedding representation is calculated based on:

<center>
$x_{emb} = word\_emb(input\_id) + pos\_emb(pos\_id) + type\_emb(type\_id)$,
</center>

where all embeddings are obtained via table lookup.

This is reflected in its [implementation](https://github.com/huggingface/transformers/blob/858545047c05a35fde437b2ada3a901844cd1e60/src/transformers/models/bert/modeling_bert.py#L210C9-L217C46):

```python
if inputs_embeds is None:
    inputs_embeds = self.word_embeddings(input_ids)
token_type_embeddings = self.token_type_embeddings(token_type_ids)

embeddings = inputs_embeds + token_type_embeddings
if self.position_embedding_type == "absolute":
    position_embeddings = self.position_embeddings(position_ids)
    embeddings += position_embeddings


#### Word Embedding  

To get the word embedding of a specific token, just look it up within the word embedding weight.

token_id = input_ids[0][1]
word_embed = bert_model.bert.embeddings.word_embeddings.weight

print("The shape of for token embedding is:", word_embed[token_id].shape)
print("The word embedding for token 'Macquarie' is:\n", word_embed[token_id])

#### Positional Embedding

Similary, the positional embedding can be obtained via table look-up.

pos_id = position_ids[0][1]
pos_embed = bert_model.bert.embeddings.position_embeddings.weight

print("The shape of for pos embedding is:", pos_embed[pos_id].shape)
print("The positional embedding for token 'Macquarie' is:\n", pos_embed[pos_id])

ðŸ’¡ **Practice:**  

Feel free to obtain embeddings for another model, such as [GPT-2](https://huggingface.co/openai-community/gpt2).

"""Load GPT-2 model and print parameter names."""

from transformers import AutoModelForCausalLM, AutoTokenizer

gpt2_model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")

print("Parameters of GPT-2 model:")
for name, param in gpt2_model.named_parameters():
    if ".h." not in name:
      print(name, param.shape)

sentence = "Macquarie University is an Australian university located in Sydney NSW."

### Tokenization results for a single sentence.
encoding = tokenizer(sentence, return_tensors="pt", return_token_type_ids=True)
input_ids = encoding["input_ids"]
print("\nInput ids is: ", input_ids)

### Q7: Please try to get the word embedding matrix of GPT-2.

token_id = input_ids[0][1]
word_embed = ...

print("The shape of for token embedding is:", word_embed[token_id].shape)
print("The word embedding for first token is:\n", word_embed[token_id])

### 4. Self-Attention Mechanism

The self-attention mechanism is the foundation of the Transformer model.

It enables each token (i.e., its embedding representation) to focus on the relevant parts of the sequence, thereby helping the model capture semantic relationships between tokens.

<center>
    <img src="https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/assets/attention.png" width="90%">
</center>

There is a tool named bertviz can be used to visualize the multi-head attention mechanism.

# !pip install bertviz

Below code cell is modified based on [this tutorial](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing):

from bertviz import head_view
from transformers import BertTokenizer, BertModel

model_version = 'bert-base-uncased'
model = BertModel.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version)

sentence = "The cat sat on the mat"

### Get the attention scores.
inputs = tokenizer.encode_plus(sentence, return_tensors='pt')
input_ids = inputs['input_ids']
attention = model(input_ids)[-1]

input_id_list = input_ids[0].tolist()
tokens = tokenizer.convert_ids_to_tokens(input_id_list)

head_view(attention, tokens)

We can also plot a heat map for the attention score directly.

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import torch

layer_index = 5
head_index = 5

attn_matrix = attention[layer_index][0, head_index].detach().numpy()

## Below is the heat map visualization code.
plt.figure(figsize=(6, 4))
ax = sns.heatmap(
    attn_matrix,
    xticklabels=tokens,
    yticklabels=tokens,
    cmap="Greens",
    vmin=0.0,
    vmax=1.0,
    square=True
)

plt.xticks(rotation=0)
plt.yticks(rotation=0)

plt.xlabel('Key Tokens', fontsize=12)
plt.ylabel('Query Tokens', fontsize=12)

plt.title(f"Layer {layer_index+1}, Head {head_index+1} Attention Score Heat Map")
plt.tight_layout()

plt.show()

ðŸ’¡ **Think about it:**  

For an encoder-only model, each token will attend to all tokens, and the attention score matrix is in a square shape.

**Question**: What will be the case for the decoder-only model?

## **This workshop aims to help you:**

- Understand the architectures of transformer models
- Understand the process of tokenization
- Understand the process of mapping tokens to embedding representation
- Farmiliar with the Self-attention Mechanism