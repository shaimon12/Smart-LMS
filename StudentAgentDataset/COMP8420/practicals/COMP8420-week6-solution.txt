# COMP8420 Advanced Natural Language Processing
## Week 6 - Prompt Engineering & RAG

## Today's Topics
- Prompt Engineering
- Retrieval-Augmented Generation (RAG)

ðŸ’¡ **NOTE**: We will want to use a GPU to accelerate when working with LLMs. In Google Colab, go to **Runtime > Change runtime type > Hardware accelerator > T4 GPU**.

### 1. Prompt Engineering



LLMs are able to generate human-like content, but sometimes their generated texts are not so good. Any method to improve it?

We can use prompt engineering to guide LLMs toward generating desired outputs for specific tasks, even without fine-tuning.

<center>
    <img src="https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/assets/chat.png" width="30%">
</center>

Figure credit: generated using ChatGPT.

#### Prompt Components

Prompt engineering is designing **natural language instructions** to guide a model toward generating **desired outputs**.

A well-crafted prompt typically consists of:

- **Instruction**: Tells the model what to do (task definition).
- **Context**: (optional) Provides background information to guide the model.
- **Query**: The specific data or question the model should respond to.

#### Example

- **Instruction**: Translate the following sentence to French.

- **Context**: French uses different word order and grammar than English.

- **Query**: I would like to order a coffee, please.

Note: The prompt engineering examples in this workshop are adapted from tutorials in the [Prompt Engineering Guide](https://www.promptingguide.ai/).

#### Zero-Shot Prompting

Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations.

In other words, we will only input **instruction** and **query** to the model.

Let's test it using some advance models via APIs (e.g., [Gemini](https://ai.google.dev/gemini-api/docs/api-key)).

!pip install -q google-generativeai

import google.generativeai as genai
import os
from IPython.display import display, Markdown
from google.generativeai.types import GenerationConfig

# Q1: Please configure your google gemini API key
api_key = 'your_api_key'
genai.configure(api_key=api_key)

# Initialize the model
model = genai.GenerativeModel('gemini-2.0-flash')

# Define a zero-shot prompt (just instruction + query)

## Below example is generated using Generative AI
instruction = "Classify the sentiment of the following text as positive, negative, or neutral."
query = "I just tried the new restaurant downtown and the food was absolutely delicious!"

# Combine instruction and query
prompt = f"{instruction}\n{query}"

# Configure generation parameters
generation_config = GenerationConfig(
    temperature=0.7,
    max_output_tokens=100, # max_output_tokens refers to max_new_tokens for gemini.
)

response = model.generate_content(prompt, generation_config=generation_config)

## Print to view the details of response.
print(response)

# Extract the text from the response
if response.candidates and len(response.candidates) > 0 and response.candidates[0].content.parts:
    text = response.candidates[0].content.parts[0].text
    display(Markdown(text))
else:
    print("No valid response text found.")

ðŸ’¡ **Practice:**
Try modifying both the instruction and query to see how the model responds:

1. **Text transformation task**:
   - Change the instruction to ask the model to transform text in a specific way (e.g., make it more formal, simplify it)
   - Provide a short paragraph as your query

2. **Structured output task**:
   - Create an instruction that asks for a specific format (e.g., bullet points, JSON)
   - Craft a query that provides information to be structured

3. **Creative writing task**:
   - Try a creative writing instruction (e.g., "Please write a poem for a given topic")
   - Craft a topic you would like the model to write about

def zero_shot_prompt(instruction, query):
    prompt = f"{instruction}\n{query}"
    response = model.generate_content(prompt)
    if response.candidates and len(response.candidates) > 0 and response.candidates[0].content.parts:
        text = response.candidates[0].content.parts[0].text
        display(Markdown(text))
    else:
        print("No valid response text found.")

## Input what you want to try.
instruction = "YOUR_TEXT_HERE"
query = "YOUR_TEXT_HERE"
zero_shot_prompt(instruction, query)

## Rephrase text in a more formal way.

## Below example is generated using Generative AI
instruction = "Rewrite the following paragraph in a more formal and academic tone."
query = "I think AI is super cool and it's gonna change how we do stuff."
zero_shot_prompt(instruction, query)

## Simpler example for Structured Output

## Below example is generated using Generative AI
instruction = "List the main features of the following smartphone in bullet points."
query = "The Galaxy S23 costs $799, has a 6.1-inch AMOLED display, 8GB RAM, 128GB storage, and a 50MP camera. It was released in February 2023."
zero_shot_prompt(instruction, query)

## Creative writing task

## Below example is generated using Generative AI
instruction = "Write a short poem about the following topic. Each line should be on its own line with proper breaks between verses."
query = "Human learning from Artificial intelligence"
zero_shot_prompt(instruction, query)

#### Few-Shot Prompting

Zero-shot prompting seems not too bad, at least the AI model reacts what we asked.

But sometimes they still give some random feedback, any method to improved?

Few-shot learning can be one such method, which give **some examples as context** to the model.

This gives the model a better understanding of the task and the output format you're looking for.

def few_shot_prompt(instruction, context, query):
    prompt = f"{instruction}\n{context}\n{query}"
    response = model.generate_content(prompt)
    if response.candidates and len(response.candidates) > 0 and response.candidates[0].content.parts:
        text = response.candidates[0].content.parts[0].text
        display(Markdown(text))
    else:
        print("No valid response text found.")

## Example of Few-Shot Prompting for Sentiment Analysis

## Below example is generated using Generative AI
instruction = "Classify the sentiment of the following text as positive, negative, or neutral."

context = """
Examples:
Text: "This movie was absolutely terrible. I want my money back."
Sentiment: Negative

Text: "The restaurant was okay, food was good but service was slow."
Sentiment: Neutral

Text: "I love the new phone! It has amazing features and the battery lasts all day."
Sentiment: Positive
"""

## Feel free to change to your own query.
query = "Text: \"The weather today is cloudy and a bit chilly, but at least it's not raining.\""

few_shot_prompt(instruction, context, query)

## Example of Few-Shot Prompting for Named Entity Recognition (NER)

## Below example is generated using Generative AI
instruction = "Extract the named entities (people, organizations, locations) from the text and classify them by type."

context = """
Examples:

Text: "Tim Cook announced Apple's new headquarters will be in Austin, Texas."
Entities:
- Tim Cook (PERSON)
- Apple (ORGANIZATION)
- Austin (LOCATION)
- Texas (LOCATION)

Text: "NASA scientists discovered water on Mars using the Perseverance rover."
Entities:
- NASA (ORGANIZATION)
- Mars (LOCATION)
- Perseverance (PRODUCT)
"""

## Feel free to change to your own query.
query = "Text: \"Jannik Sinner defeated Alexander Zverev and successfully defended his men's singles title at the 2025 Australian Open in Melbourne Park.\""

few_shot_prompt(instruction, context, query)

## Example of Few-Shot Prompting for Question Answering

## Below example is generated using Generative AI
instruction = "Answer the question based only on the information provided in the text."

context = """
Examples:

Text: "The Great Barrier Reef, located off the coast of Queensland in northeastern Australia, is the world's largest coral reef system. It stretches for over 2,300 kilometers and can be seen from outer space."
Question: "How long is the Great Barrier Reef?"
Answer: The Great Barrier Reef stretches for over 2,300 kilometers.

Text: "Marie Curie was a Polish-born physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize and the only person to win Nobel Prizes in multiple scientific fields (Physics and Chemistry)."
Question: "What nationality was Marie Curie?"
Answer: Marie Curie was Polish-born.
"""

## Feel free to change to your own query.
query = """
Text: "The Sydney Harbour Bridge, completed in 1932, is a steel through arch bridge across Sydney Harbour. It carries rail, vehicular, bicycle, and pedestrian traffic between the Sydney central business district and the North Shore. The dramatic view of the bridge, the harbour, and the nearby Sydney Opera House is an iconic image of Sydney and Australia."
Question: "Where is the Sydney Harbour Bridge located?"
"""

few_shot_prompt(instruction, context, query)

#### Chain-of-thought Prompting

Sometimes, we don't just want an answer â€” we want the **thinking process** behind it.

Can we ask the model to **show its reasoning steps**?

Yes! With **chain-of-thought (CoT) prompting**, we guide the model to break down its thinking step by step before reaching a final answer. This is especially useful for:
- Math and logic problems
- Commonsense reasoning
- Multi-hop question answering

In CoT prompting, we explicitly include phrases like _"Let's think step by step."_ or give **an example of how reasoning unfolds**.

import time

def streaming_cot_prompt_chunked(instruction, examples, query, typewriter_effect=False):
    prompt = f"{instruction}\n\n{examples}\n\n{query}"

    response = model.generate_content(prompt, stream=True)

    print("Model is thinking...")

    full_response = ""
    for chunk in response:
        if hasattr(chunk, 'text') and chunk.text:
            full_response += chunk.text

            if typewriter_effect:
              print(chunk.text, end="", flush=True)
              time.sleep(0.2) #added a small delay between chunks to make it like a typerwriter.

    if not typewriter_effect:
      print("Complete Output:\n")
      display(Markdown(full_response))

## Below example is generated using Generative AI

instruction = "Solve the following math problem by showing your reasoning step by step."

examples = """
Examples:

Problem: Sarah has 5 apples. She buys 2 more apples and then gives 3 apples to her friend. How many apples does Sarah have now?
Thinking:
1. Initially, Sarah has 5 apples.
2. She buys 2 more apples, so now she has 5 + 2 = 7 apples.
3. She gives 3 apples to her friend, so she has 7 - 3 = 4 apples left.
Answer: Sarah has 4 apples.

Problem: A train travels at 60 km/h for 2 hours, then at 40 km/h for 3 hours. How far does it travel in total?
Thinking:
1. For the first 2 hours, the train travels at 60 km/h.
   Distance = speed Ã— time = 60 km/h Ã— 2 h = 120 km.
2. For the next 3 hours, the train travels at 40 km/h.
   Distance = speed Ã— time = 40 km/h Ã— 3 h = 120 km.
3. Total distance = 120 km + 120 km = 240 km.
Answer: The train travels 240 km in total.
"""

query = "Problem: Tom is planning a party. He buys 3 boxes of balloons, with 12 balloons in each box. If he already had 5 balloons and needs 40 balloons for the party, how many extra balloons will he have?"

streaming_cot_prompt_chunked(instruction, examples, query, typewriter_effect=True)

instruction = "Solve the following math problem by showing your reasoning step by step."
examples=""
query = "Problem: Tom is planning a party. He buys 3 boxes of balloons, with 12 balloons in each box. If he already had 5 balloons and needs 40 balloons for the party, how many extra balloons will he have?"

streaming_cot_prompt_chunked(instruction, examples, query, typewriter_effect=False)

#### System instructions & Multi-Turn Conversation

What else can we do?

You can even let the model pretend it is someone, and engage with it for multi-turn conversation!

For google gemini, they provide a [Chat API](https://ai.google.dev/gemini-api/docs/text-generation#multi-turn-conversations) to enable multiple-turn conversation.

from google import genai
from google.genai import types
from IPython.display import display, Markdown

client = genai.Client(api_key="your_api_key")
chat = client.chats.create(
    model="gemini-2.0-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a cat. Your name is Neko. Remember to say meow for your every response."
    )
)

def send_and_display_message(chat, prompt):
    """Sends a message to the chat and displays the response as Markdown."""
    response = chat.send_message_stream(prompt)
    full_response = "".join(chunk.text for chunk in response)
    display(Markdown(full_response))

def format_chat_history(chat, model_name):
    """Formats and displays the chat history as a conversational string."""
    formatted_history = ""
    current_role = None
    for message in chat.get_history():
        if current_role != message.role:
            if current_role is not None:
                formatted_history += "\n"
            current_role = message.role
            if current_role == f"{model_name}":
                formatted_history += "Neko: "
            else:
                formatted_history += "Me: "
        formatted_history += message.parts[0].text + "\n"
    display(Markdown(formatted_history))

# Construct the initial prompt
context = "There is a fish on the table."
query = "What will you do?"
full_prompt = f"Context: {context}\n\nQuery: {query}"

# Send and display the first message
send_and_display_message(chat, full_prompt)

# Send and display the second message
query = "But this is for the kids at home. What do you think?"
send_and_display_message(chat, query)

# Display the formatted chat history
format_chat_history(chat, model_name='Neko')

ðŸ’¡ **Practice:**

Create an LLM-based chatbot with your characteristics.

Feed it your quirks, hobbies, work, petsâ€”make it personal! Then, chat with it, or let the friend next to you have a try!

from google import genai
from google.genai import types
from IPython.display import display, Markdown

chat = client.chats.create(
    model="gemini-2.0-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are pretending to be [Your Name]. Respond as if you were [Your Name]."
    )
)

# Initial context message
personal_info = """
My name is [Your Name]. I enjoy [Your Hobbies]. I work as a [Your Job]. I have a pet [Your Pet].
"""

send_and_display_message(chat, personal_info)

# Send a new message
send_and_display_message(chat, "What do you think of [Your Question]?")

# Print the conversation
format_chat_history(chat, model_name='[Your Name]')

#### 2. Retrieval-Augmented Generation (RAG)

Prompt engineering lets us guide how the model responds â€” but the model still only knows what it was trained on.

What if the model doesn't know something?
- What's in the latest course unit guide?
- What is being taught in today's lecture?

That's when we need **RAG**: **give the model external knowledge at runtime**.


Figure credit: [Cohere AI Github](https://github.com/cohere-ai/cohere-developer-experience/tree/main).

<center>
    <img src="https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/images/llmu/rag/rag-workflow-1.png?raw=true" width="80%">
</center>

ðŸ’¡ **Think About It:**

In the RAG pipeline, the model interacts with an external database to retrieve relevant documents based on the query.

But a key question remains:

**How do we determine which documents to retrieve?**

Hints:
- What elements do we compare between the query and potential documents?
  - Ans: We compare query and documents using **feature representations** like TF-IDF, BOW (sparse retrievers), or **embeddings** (dense retrievers).
  - (Optional reading) [Dense Passage Retrieval (DPR)](https://arxiv.org/pdf/2004.04906v2), a notable dense embedding retrieval system introduced by Facebook in 2020.
- What metrics do we use to make retrieval decisions?
  - Ans: We measure similarity between the query and documents using metrics such as **cosine similarity** or **dot product**.

from sentence_transformers import SentenceTransformer, util

embedder = SentenceTransformer("all-MiniLM-L6-v2")

### Below example is adapted based on a Cohere RAG tutorial: https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/llmu/Introduction_to_RAG.ipynb
documents = [
    "Emperor penguins are the tallest penguins.",
    "Emperor penguins live only in Antarctica.",
    "Animals are not the same as plants.",
    "Cars are used to travel long distances.",
    "Apples can be red, green, or yellow.",
]

# Embed the docs
doc_embeddings = embedder.encode(documents, convert_to_tensor=True)
print("Shape of the document embeddings: ", doc_embeddings.shape)

Question: What is the shape of the sentence embedding, and why?  
Answer: Because the embedder uses mean pooling across tokens by default.

print(embedder)

def retrieve_indirect_context(question, top_k=2, threshold=0.1):
    query_embedding = embedder.encode(question, convert_to_tensor=True)
    cos_scores = util.cos_sim(query_embedding, doc_embeddings)[0]

    results = []
    for i, score in enumerate(cos_scores):
        score_float = float(score)
        # Only include sentence with similarity higher than the threshold
        if score_float > threshold:
            results.append((i, score_float))

    results = sorted(results, key=lambda x: x[1], reverse=True)[:top_k]

    return [documents[i] for i, _ in results]

query = "What are the tallest living penguins, and where do they live?"
context = retrieve_indirect_context(query)
for sentence in context:
  print(f"{sentence}\n")

import google.generativeai as genai
from IPython.display import display, Markdown

api_key = 'your_api_key'
genai.configure(api_key=api_key)
model = genai.GenerativeModel('gemini-2.0-flash')

prompt = f"Context: {context}\n\nQuestion: {query}"

# Generate answer with Gemini
response = model.generate_content(prompt)
display(Markdown(response.text))

ðŸ’¡ **Reflection:**

RAG is a natural extension of prompt engineering â€” instead of relying on your prompt alone, you enhance it with the right knowledge at the right time.

## **This workshop aims to help you:**

- Understand how to **steer LLMs without training** using techniques like **zero-shot**, **few-shot**, **chain-of-thought**, and **role-based prompting**.
- Understand using **Retrieval-Augmented Generation (RAG)** to bring external knowledge into LLMs at runtime â€” enabling the model to answer questions it wasn't originally trained for.