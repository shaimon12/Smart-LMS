# COMP8420 Advanced Natural Language Processing
## Week 4 - LLMs for Text Processing

## Today's Topics
- Text Classification
- Text Generation


ðŸ’¡ **NOTE**: We will want to use a GPU to accelerate when working with LLMs. In Google Colab, go to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.

In Week 2, we discuss NLP techniques designed to deal with real-world tasks, such as:
- **Text classification** (e.g., movie/product sentiment analysis, email spam detection), such as [Stanford Sentiment Treebank v2 (SST-2)](https://huggingface.co/datasets/stanfordnlp/sst2).
- **Text generation** (e.g., chatbots like ChatGPT, Gemini, Claude), such as [Stanford Alpaca](https://huggingface.co/datasets/yahma/alpaca-cleaned).
- ...

Today, we will be exploring how to use Transformer models to handle these tasks.

!pip install datasets

### 1. Text Classification

Text Classification is the task of assigning a label or class to a given text.

We have explored some use cases, such as **binary sentiment analysis**:

"""Load SST-2 dataset from HuggingFace"""

from datasets import load_dataset
from pprint import pprint

sst2 = load_dataset("stanfordnlp/sst2")
pprint(sst2['validation'][0], width=100)

We also explored using HuggingFace **Pipeline** abstraction to perform inference for given input data:

"""Pipeline inference for given sentence"""

from transformers import pipeline

cls_pipe = pipeline("text-classification", model="JeremiahZ/bert-base-uncased-sst2")
cls_pipe("I love this movie!")

Or adapting text classification pipeline on given datasets:

"""Example of text classification pipeline"""

from transformers import pipeline
from sklearn.metrics import classification_report

# Initialize the classification pipeline
cls_pipe = pipeline("text-classification", model="JeremiahZ/bert-base-uncased-sst2")

# Get predictions for all validation samples
validation_texts = [item['sentence'] for item in sst2['validation']]
predictions = cls_pipe(validation_texts, batch_size=32)

true_labels = [item['label'] for item in sst2['validation']]
predicted_labels = [1 if pred['label'] == 'positive' else 0 for pred in predictions]

report = classification_report(true_labels, predicted_labels, target_names=['negative', 'positive'])
print(report)

#### Beyond Binary Sentiment Analysis

Besides binary sentiment analysis, common text classification tasks also include:

1. **Multi-class classification**: Allocates a given text to one of multiple classes.
2. **Natural Language Inference (NLI)**: Determines the relationship between two given texts (e.g., entailment or contradiction).

Example: Multi-class News Classification ([AG News](https://huggingface.co/datasets/fancyzhx/ag_news))

from datasets import load_dataset
import pandas as pd

# Load AG News datasets
agnews = load_dataset("fancyzhx/ag_news")
shuffled_train = agnews['train'].shuffle(seed=666)

agnews_df = pd.DataFrame(shuffled_train[:10])
agnews_df['label_name'] = agnews_df['label'].map({
    0: 'World',
    1: 'Sports',
    2: 'Business',
    3: 'Sci/Tech'
})

# Set the column display width for the dataframe
pd.set_option('display.max_colwidth', 200)
pd.set_option('display.width', 1000)

agnews_df[['text', 'label', 'label_name']].head(5)

#### Example: Natural Language Inference ([SNLI](https://huggingface.co/datasets/stanfordnlp/snli))

"""Loading SNLI dataset for natural language inference"""
from datasets import load_dataset
import pandas as pd

# Load SNLI dataset
snli = load_dataset("snli")

# Create a DataFrame for better visualization
snli_df = pd.DataFrame(snli['train'][:5])

# Map numerical labels to their meanings
snli_df['label_name'] = snli_df['label'].map({
    0: 'entailment',
    1: 'neutral',
    2: 'contradiction'
})

# Display sample data
snli_df.head()

Could we also use **Pipeline** to handle NLI classification? With two input texts, how might we approach this?

# Initialize the SNLI classification pipeline
snli_cls = pipeline("text-classification", model="roberta-large-mnli")

# Question 1: The pipeline expects a single sentence as input, but SNLI provides two (premise and hypothesis).
# How should we process the input data?
def process_example(example):
    # return ...
    # Option 1: Simple concatenation (may work but not ideal for some models)
    return example['premise'] + " " + example['hypothesis']

    # Option 2: Use BERT-style separator (for BERT models)
    # return example['premise'] + " [SEP] " + example['hypothesis']

    # Option 3: Use RoBERTa-style separator (preferred for RoBERTa models)
    # return example['premise'] + " </s> " + example['hypothesis']

# Example: Inspect and process a sample from the SNLI validation set
idx = 100
example_df = pd.DataFrame(snli['validation'][idx:idx + 1])
display(example_df)

input_text = process_example(example_df.iloc[0])
snli_cls(input_text)

#### Classification with Decoder-only Model

Decoder-only models can also be effectively adapted for text classification tasks by using the appropriate model head:
- [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification): Optimized for text classification tasks
- [AutoModelForCausalLM](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM): Designed for generative tasks like text generation and language modeling

Even though decoder-only models like GPT-2 are primarily pre-trained for causal language modeling, they can be equipped with a sequence classification head to handle classification tasks effectively.

For example, GPT-2 has both two types of head class:
- [GPT2ForSequenceClassification](https://github.com/huggingface/transformers/blob/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src/transformers/models/gpt2/modeling_gpt2.py#L1332)
- [GPT2LMHeadModel](https://github.com/huggingface/transformers/blob/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src/transformers/models/gpt2/modeling_gpt2.py#L978)

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

test_example = sst2["validation"][0]

## Load GPT2 model with SEQ_CLS head.
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForSequenceClassification.from_pretrained("gpt2",num_labels=2)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

print("Making prediction...")
inputs = tokenizer(test_example["sentence"], return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model(**inputs)
logits = outputs.logits
predicted_class = torch.argmax(logits, dim=1).item()
probabilities = torch.nn.functional.softmax(logits, dim=1)

print(f"\nInput: {test_example['sentence']}")
print(f"True label: {test_example['label']} (0=negative, 1=positive)")
print(f"Predicted class: {predicted_class} (0=negative, 1=positive)")
print(f"Confidence: {probabilities[0][predicted_class].item():.4f}")

ðŸ’¡ **Note**:

Here, we just explore how to use a decoder-only model for text classification.

For practical applications, feel free to fine-tune the model on your target dataset (e.g., SST-2) to achieve better results.

### 2. Text Generation

Text generation is the task of producing new text that continues or completes a given input text.

Text generation works in an **autoregressive fashion**, where each newly predicted token is appended to the input and then used to generate **subsequent tokens iteratively**. This process continues until the desired length is reached or a stopping condition (like `<endoftext>`) is met.

The figure below demonstrates text generation by showing how a model generates the First Law of Robotics.

*Credit: Alammar, J. (2019). The Illustrated GPT-2 [Blog post]. Retrieved from http://jalammar.github.io/illustrated-gpt2/*



<center>
    <img src="http://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif" width="70%">
</center>

In today's workshop, we will explore four approaches to working with decoder-only LLMs for text generation tasks:

- HuggingFace Text Generation Pipeline
- HuggingFace Custom Model Loading
- Advanced ChatBot APIs
- Local Deployment via Ollama

#### 2.1 HuggingFace Text Generation Pipeline

Using the text-generation pipeline is as straightforward as working with the text-classification task:

from transformers import pipeline

pipe = pipeline("text-generation", model="gpt2")
output = pipe("Once upon a time, there was a", max_new_tokens=50)

print(output)

Accessing models on Hugging Face is generally easy, but some models require additional steps, such as credential authentication.  

**Uncomment the lines below to check potential issues that may arise without authentication:**

# llama_pipe = pipeline("text-generation", model="meta-llama/Llama-3.2-1B")
# output = llama_pipe("Once upon a time, there was a", max_new_tokens=50)

For advanced models like LLaMA and Gemma, access grants and authentication with a Hugging Face token are required. The process typically involves three steps:  

1. **Apply for model access**, e.g., visit the [LLaMA-3.2-1B model page](https://huggingface.co/meta-llama/Llama-3.2-1B).  
2. **Create a Hugging Face access token** via [Hugging Face settings](https://huggingface.co/settings/tokens).  
3. **Authenticate with Hugging Face Hub** using one of the following methods:  
   - **Recommended:** [Hugging Face CLI login](https://huggingface.co/docs/huggingface_hub/en/guides/cli#huggingface-cli-login).  
   - Set the environment variable: `export HF_TOKEN=<YOUR_ACCESS_TOKEN>`.  
   - Authenticate via Python API (example below).  

# This method is simple but exposes your token in plaintext.
# Always remember to remove it before sharing your script.

# Practice: please setup and input your access token.
from huggingface_hub import login
login(token="YOUR ACCESS TOKEN")

llama_pipe = pipeline("text-generation", model="meta-llama/Llama-3.2-1B")
llama_pipe("Once upon a time, there was a", max_new_tokens=50)

ðŸ’¡ **Practice:**  

Try entering any sentence into the pipeline and check the output in the cell below:

llama_pipe("Feel free to enter any text here...", max_new_tokens=50)

#### 2.2 HuggingFace Custom Model Loading

We can also load models directly from Hugging Face for more flexibility in text generation.

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed

set_seed(42)

# Load tokenizer and model
model_name = "meta-llama/Llama-3.2-1B"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

def query(model, tokenizer, instruction):
    prompt = f"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # The generate function supports various tunable hyperparameters.
    # Practice: feel free to change these values and explore the impact.
    with torch.no_grad():
        output = model.generate(
            **inputs,
            top_p=0.9,
            temperature=0.7,
            do_sample=True,
            max_new_tokens=100,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
        )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    print("The complete model output is:\n", decoded)
    # Return the model's response after the "### Response:" marker
    return decoded.split("### Response:")[1].strip()

# Example query
instruction="Please tell me a short fairy tale."
response = query(model, tokenizer, instruction)
print("\n================= This is a delimiter =================\n")
print("The model response is:\n", response)

ðŸ’¡ **Note:**  

There are many hyperparameters you can tune to adjust the modelâ€™s generation behavior.  
Feel free to explore the Hugging Face documentation on [GenerationConfig](https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/text_generation#transformers.GenerationConfig) and experiment with them.

#### 2.3 Advanced ChatBot APIs

We can also leverage advanced language models like **GPT**, **Claude**, and **Gemini** through their APIs.  

In this example, we demonstrate the **Gemini API**, which currently offers a free-tier service.  

To obtain a Gemini API key, visit: [https://ai.google.dev/gemini-api/docs](https://ai.google.dev/gemini-api/docs).

## Install the package for using the Gemini API
# !pip install -q -U google-generativeai

"""Chat with Gemini via API."""

from google import genai
from IPython.display import Markdown, display

# Practice: please setup and input your gemini api key.
client = genai.Client(api_key="YOUR API KEY")

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents="Explain how AI works, keep the response in concise bullet points.", # Feel free to change the prompt.
)

display(Markdown(response.text))

ðŸ’¡ **Note:**  

Feel free to explore other advanced LLM APIs, such as:  
- [OpenAI API (GPT models)](https://platform.openai.com/docs/api-reference/chat/create)  
- [Claude API (Anthropic)](https://docs.anthropic.com/en/docs/initial-setup)

#### 2.4 Local Deployment via Ollama

Alternatively, you can deploy open-source models on your own hardware.  

**Ollama** is an open-source framework that provides easy local deployment for models like **LLaMA**, **Gemma**, **Mistral**, and others. It is primarily designed for local environments but can also run on **Google Colab** ([source](https://medium.com/@abonia/running-ollama-in-google-colab-free-tier-545609258453)).  

### Getting Started:  
1. Download Ollama from [https://ollama.com/download](https://ollama.com/download).
2. Install the `ollama` Python package.

## Install the Ollama python package.
!pip install ollama

# Install the Ollama backend

!curl https://ollama.ai/install.sh | sh

Start the Ollama server and pull (download) a model:  

!ollama serve > server.log 2>&1 &

!ollama pull gemma3:1b

Once the model is ready, you can interact with it easily ([using python](https://github.com/ollama/ollama-python)):

import ollama
from IPython.display import Markdown, display

def chat(prompt):
    client = ollama.Client(host='http://localhost:11434')
    response = client.chat(model="gemma3:1b", messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

response = chat("Please explain the concept of machine learning.")
display(Markdown(response))



ðŸ’¡ **Practice:**  
- Feel free to chat with the model using `chat("Your prompt here")`.  
- You can also try installing Ollama on your local machine and experimenting with different models.  

## **This workshop aims to help you:**  

- Understand how to handle various text classification tasks.  
- Become familiar with different methods of interacting with decoder-only LLMs:  
  - Inference using the Hugging Face pipeline  
  - Loading models directly from Hugging Face  
  - Chatting with advanced models through APIs  
  - Running LLMs locally via Ollama