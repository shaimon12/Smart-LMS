# COMP8420 Advanced Natural Language Processing
## Week 5 - Multi-modal Models

## Today's Topics
- Text-Vision Understanding (e.g., CLIP)
- Text-Vision Generation (e.g., BLIP)
- Text-Vision Conversation (e.g., LLaVA)

ðŸ’¡ **NOTE**: We will want to use a GPU to accelerate when working with LLMs. In Google Colab, go to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.

### 1. Text-Vision Understanding (e.g., CLIP)

Contrastive Language-Image Pre-training (CLIP) ([Paper](https://arxiv.org/pdf/2103.00020), [Intro](https://openai.com/index/clip/?ref=blog.roboflow.com) and [Code](https://github.com/openai/CLIP/blob/dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1/clip/model.py#L243)) is a neural network that learns visual and language concepts simultaneously.

It consists of two components: a text encoder ($f_{\text{text}}$) and an image encoder ($f_{\text{image}}$).

During pre-training with (image, text) input pairs, its [objective (loss function)](https://github.com/openai/CLIP/blob/dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1/clip/model.py#L366) to maximize:
$$\text{similarity}(f_{\text{image}}(\text{image}), f_{\text{text}}(\text{text}))$$

This approach builds joint understanding across both modalities.

<center>
    <img src="https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png" width="90%">
</center>

#### Architecture of CLIP

We'll first load a [CLIP model](https://huggingface.co/openai/clip-vit-base-patch16) to examine its architecture and explore how it learns joint language and vision semantics.

from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch16")  ## What does this model name suggest?
# Note: 'vit-base-patch16' indicates Vision Transformer architecture, 'base' is the model size,
# and 'patch16' means each image patch has a width of 16 pixels.

Let's examine the architecture of CLIP by printing its parameters:

for name, param in model.named_parameters():
  if 'embeddings' in name or 'projection' in name:
    print(name, param.shape)

ðŸ’¡ **Think about it**:
- Q1: What modules are used in the text encoder and vision encoder to process input data?
  - A1:
    - Text-encoder: token_embedding, position_embedding
    - Vision-encoder: class_embedding, patch_embedding, position_embedding
- Q2: What are the final outputs used to calculate similarity, and what are their shapes?
  - A2: Both text and vision encoders use projection layers to map representations into a shared 512-dimensional embedding space.

Let's also check one example of how the text model encodes text input:

from transformers import CLIPProcessor, CLIPTokenizer

## Load the tokenizer.
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch16")
example_text = "Pepper the aussie pup"

## Encode the input text.
token_ids = tokenizer.encode(example_text)
print("Token IDs:", token_ids)

## Map the token ids back to tokens.
tokens = tokenizer.convert_ids_to_tokens(token_ids)
print("Tokens:", tokens)

ðŸ’¡ **Important insight**:   
The text input has 6 tokens, and after processing, the text encoder produces a representation with shape (6,512). But which part of this output is used for computing similarity with images?

You can find the answer from its original implementation:
- Text encoder: [code](https://github.com/openai/CLIP/blob/dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1/clip/model.py#L353)
- Vision encoder: [code](https://github.com/openai/CLIP/blob/dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1/clip/model.py#L235)

(Answer) CLIP doesn't use all token embeddings for the final similarity calculation:
- **Text encoder**: CLIP uses the embedding of the `<|endoftext|>` token as the final text representation vector
- **Vision encoder**: CLIP uses the embedding of the class token (first position) as the final image representation vector

#### Zero-Shot Image Classification

A key feature of CLIP models is their zero-shot classification capability, which allows them to handle **data never seen during training**.

Let's explore an example using AI-generated images:

"""Load AI-generated images and some captions to explore zero-shot classification"""

from urllib.request import urlopen
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from transformers import CLIPProcessor, CLIPModel

# Load model and processor
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch16")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch16")

# Image URLs and captions
image_urls = [
    "https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/assets/ai-generated/dog.png",
    "https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/assets/ai-generated/cat.png",
    "https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/assets/ai-generated/car.png"
]

captions = [
    "a golden retriever playing with a red ball",
    "a white cat sitting next to a cup of coffee",
    "a red sports car speeding on a highway"
]

images = [Image.open(urlopen(path)).convert("RGBA") for path in image_urls]

# Plot the images
fig, axes = plt.subplots(1, len(image_urls), figsize=(9, 3))

for idx, image in enumerate(images):
    axes[idx].imshow(image)
    axes[idx].axis('off')

plt.tight_layout()
plt.show()

Next, we will extract both text and image embeddings using the CLIP model:

import numpy as np

# Embed all images
image_embeddings = []
for image in images:
  image_inputs = processor(images=image, return_tensors='pt')
  image_embed = model.get_image_features(**image_inputs).detach().cpu().numpy()[0]
  image_embeddings.append(image_embed)
image_embeddings = np.array(image_embeddings)

# Embed all captions
text_embeddings = []
for caption in captions:
  text_inputs = processor(text=caption, return_tensors="pt")
  text_emb = model.get_text_features(**text_inputs).detach().cpu().numpy()[0]
  text_embeddings.append(text_emb)
text_embeddings = np.array(text_embeddings)

CLIP calculates cosine similarity to find the best-matching pairs. The cosine similarity formula is:

$$\text{cosine similarity} = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \cdot ||\mathbf{B}||}$$

def cosine_similarity_manual(image_embeddings, text_embeddings):
    # We've calculated the L2 norms for you
    image_norm = np.linalg.norm(image_embeddings, axis=1, keepdims=True)
    text_norm = np.linalg.norm(text_embeddings, axis=1, keepdims=True)

    # TODO: Normalize the embeddings by dividing by their L2 norms
    # image_embeddings_normalized = # Your code here
    # text_embeddings_normalized = # Your code here

    # Answer
    image_embeddings_normalized = image_embeddings / image_norm
    text_embeddings_normalized = text_embeddings / text_norm

    # TODO: Compute dot product between normalized vectors to get cosine similarity
    # similarity_matrix = # Your code here

    # Answer
    similarity_matrix = np.dot(image_embeddings_normalized, text_embeddings_normalized.T)

    return similarity_matrix

sim_matrix = cosine_similarity_manual(image_embeddings, text_embeddings)

## Alternatively, you can use sklearn's implementation.
# from sklearn.metrics.pairwise import cosine_similarity
# sim_matrix = cosine_similarity(image_embeddings, text_embeddings)

Finally, let's visualize the similarity matrix:

# Create base figure
plt.figure(figsize=(20, 10))

# Create visualization
plt.imshow(sim_matrix, cmap='viridis')

# Adjust ticks with correct labels
plt.yticks(range(len(captions)), captions, fontsize=16)
plt.xticks([])

# Visualize
for i, image in enumerate(images):
    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin="lower")

# Add the captions at the correct indices
for x in range(sim_matrix.shape[1]):
    for y in range(sim_matrix.shape[0]):
        plt.text(x, y, f"{sim_matrix[y, x]:.2f}", ha="center", va="center", size=24)

# Remove unnecessary spines
for side in ["left", "top", "right", "bottom"]:
  plt.gca().spines[side].set_visible(False)

# Resize blocks
plt.xlim([-0.5, len(captions) - 0.5])
plt.ylim([len(captions) - 0.5, -1.6])

plt.tight_layout()
plt.show()

ðŸ’¡ **Takeaways**:

- **Zero-shot transfer**: CLIP generalizes to unseen image-text pairs, unlike traditional CV models needing retraining for new labels.  
- **Semantic alignment**: CLIP aligns visual and language features, powering retrieval systems and text-to-image models (e.g., Stable Diffusion).  
- **Cross-modal convergence** *(optional reading)*: Different modalities can naturally align without CLIP-style trainingâ€”see [ICML 2024 paper](https://r.jordan.im/download/technology/huh2024.pdf).

### 2. Text-Vision Generation (e.g., BLIP)

ðŸ”Ž **Beyond CLIP**: While CLIP focuses on vision-language understanding and matching, can we bridge the modality gap even further?  

âœ… **Yes â€” with BLIP** ([Paper](https://arxiv.org/pdf/2201.12086)): By introducing an additional text encoder and decoder, BLIP enables powerful **image-to-text generation** capabilities, going beyond retrieval to support tasks like **Image Captioning** and **Vision Question Answering**.

<center>
    <img src="https://production-media.paperswithcode.com/methods/bf9bd9a4-da80-4059-bdc1-3f49549d4044.png" width="80%">
</center>

**The BLIP Architecture** (3 pretraining tasks, 4 components):  
- **ITC (Image-Text Contrastive)**: Aligns image and text features, similar to CLIP (**image encoder + text encoder**).  
- **ITM (Image-Text Matching)**: Improves fine-grained matching with **image encoder + text encoder (cross-attention)**.  
- **LM (Language Modeling)**: Enables text generation from images using **image encoder + text decoder**.  

âœ… **Pretraining** involves all three tasks.  
âœ… **Inference** uses only **LM** for generating text from images.

#### Image Captioning Demo

Example: Load [BLIP](https://huggingface.co/Salesforce/blip-image-captioning-base) for image captioning and test on any image.

import torch
import requests
from PIL import Image
import matplotlib.pyplot as plt
from transformers import BlipProcessor, BlipForConditionalGeneration

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

## Load BLIP image captioning model.
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base", torch_dtype=torch.float16).to(device)

## Feel free to change to any image url
img_url = 'https://images.unsplash.com/photo-1728044849280-10a1a75cff83?q=80&w=3870&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDF8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D'
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')

## Plot the image
plt.figure(figsize=(8, 6))
plt.imshow(raw_image)
plt.axis('off')
plt.show()

## Preprocess input
inputs = processor(raw_image, return_tensors="pt").to(device, torch.float16)
print("Processed input keys:", inputs.keys())

## Generate caption
out = model.generate(**inputs)
print("\nGenerated token IDs:", out[0])
print("Decoded output:", processor.decode(out[0], skip_special_tokens=False))
print("Generated Caption:", processor.decode(out[0], skip_special_tokens=True))

ðŸ’¡ **Practice**:

- You can replace the image URL to test captioning on different images (such as images from [unsplash](https://unsplash.com/)).

#### Visual Question Answering

Example: Load [BLIP VQA model](https://huggingface.co/Salesforce/blip-vqa-base) and test on any image.

import torch
import requests
from PIL import Image
from transformers import BlipProcessor, BlipForQuestionAnswering

processor = BlipProcessor.from_pretrained("ybelkada/blip-vqa-base")
model = BlipForQuestionAnswering.from_pretrained("ybelkada/blip-vqa-base", torch_dtype=torch.float16).to(device)

img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')

## Plot the image
plt.figure(figsize=(8, 6))
plt.imshow(raw_image)
plt.axis('off')
plt.show()

## Feel free to change the question
question = "how many dogs are there in the picture?"
inputs = processor(raw_image, question, return_tensors="pt").to(device, torch.float16)

out = model.generate(**inputs)
print(processor.decode(out[0], skip_special_tokens=True))

ðŸ’¡ **Practice**:

- You can replace the image URL or question to explore VQA performance on different examples.

### 3. Text-Vision Conversation (e.g., LLaVA)

What if we want multi-turn conversations grounded in images?

**LLaVA** ([Paper](https://arxiv.org/pdf/2304.08485.pdf)) tackles this by connecting a **vision encoder** to a **large language model (LLM)**, enabling the model to **see images and chat naturally**.

<center>
    <img src="https://llava-vl.github.io/images/llava_arch.png" width="70%">
</center>

**LLaVA Architecture**:
- **Projects image features** into the LLMâ€™s embedding space.
- The LLM handles both **dialogue history** and **image-grounded reasoning**, supporting multi-turn conversations.

Since models like LLaVA are typically large, weâ€™ll use **Ollama** to load a small variant (`llava-phi3-int4`) and explore its functionality.

#### ðŸ›  Install and Launch Ollama with LLaVA

## Install the Ollama python package.
!pip install ollama

# Install the Ollama backend
!curl https://ollama.ai/install.sh | sh

Launch the ollama server and download the `llava-phi3-int4` model:

!ollama serve > server.log 2>&1 &

!ollama pull llava-phi3

Load and Display an Image:

import requests
import base64
from PIL import Image
from io import BytesIO
from matplotlib import pyplot as plt
import json

# URL of the image to fetch
image_url = "http://blog-imgs-94.fc2.com/k/a/b/kabochan112/blog_import_57a6373854a8b.jpg"

# Fetch image data from the URL
image_data = requests.get(image_url).content

# Open the image using PIL (Python Imaging Library)
PIL_image = Image.open(BytesIO(image_data))

# Plot and display the image using matplotlib
plt.figure(figsize=(8, 4))
plt.imshow(PIL_image)
plt.axis('off')  # Hide axes
plt.show()

Define a Chat Function with LLaVA:

# Helper function to interact with the llava-phi3 model
# Sends the image along with a prompt and returns the model's response
def ask_llava(prompt, image_data, model="llava-phi3"):
    # Construct payload for API request
    payload = {
        "model": model,
        "prompt": prompt,
        "images": [base64.b64encode(image_data).decode('utf-8')]
    }

    # Send request to local llava-phi3 model API
    response = requests.post("http://localhost:11434/api/generate", json=payload)

    # Process and return the response from the model
    return ''.join(
        json.loads(line).get('response', '')
        for line in response.text.strip().split('\n') if line
    )

Run the conversation:

from IPython.display import display, Markdown

# Example prompt to describe the image
prompt = "Please describe this image."

# Get the description of the image from llava-phi3 model
description = ask_llava(prompt, image_data)

# Print out the generated description
display(Markdown(f"**Image Description:**\n\n{description}"))

specific_question = ask_llava("What breed is the animal in this image?", image_data)
display(Markdown(f"**Answer:**\n\n{specific_question}"))

ðŸ’¡ **Practice**:  
- You can continue chatting with the LLaVA model by calling `ask_llava()` with new questions.
- Try asking about objects, colors, or reasoning about the scene!



## **This workshop aims to help you:**  

- Understand the concepts of **vision-language understanding**, **generation**, and **conversation** tasks.  
- Explore models like **CLIP** for text-vision matching and retrieval.  
- Generate captions and answers from images using models like **BLIP**.  
- Interact with **multi-modal conversational models** like **LLaVA** that combine vision and language capabilities.  