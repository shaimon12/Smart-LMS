# COMP8420 Advanced Natural Language Processing
## Week 7 - Fine-tunning LLM

## Today's Topics
- LLM Supervised Fine-Tuning (SFT) and Low-Rank Adaptation (LoRA)
- Human Preference Optimization (RLHF (PPO) & DPO)

ðŸ’¡ **NOTE**: We will want to use a GPU to accelerate when working with LLMs. In Google Colab, go to **Runtime > Change runtime type > Hardware accelerator > T4 GPU**.

#### 1. LLM Supervised Fine-Tuning (SFT) and Low-Rank Adaptation (LoRA)

Last week, we explore prompting and RAG, which enable us steer models **without changing their weights**.

But sometimes we want the model to:
- Learn a **new task**
- Speak in a **specific tone**
- Adapt to a **domain** permanently

That's when we fine-tune.

Figure source: https://neo4j.com/blog/developer/fine-tuning-vs-rag/

<center>
    <img src="https://dist.neo4j.com/wp-content/uploads/20230608064931/1LTfNOqZQB_M42KyvttjSyw.png" width="65%">
</center>


To fine-tune LLMs, you typically need:
- A well-formatted dataset (e.g., instruction â†’ response)
- Sufficient compute to train or adapt the model

We will first try to load a Question Answering dataset and change it to a suitable format.  
Here, we use [nvidia/helpsteer](https://huggingface.co/datasets/nvidia/HelpSteer) as an example.

!pip install datasets
!pip install -q -U bitsandbytes
!pip install -q -U trl fsspec==2025.3.2
!pip install -q -U git+https://github.com/huggingface/transformers.git
!pip install -q -U git+https://github.com/huggingface/peft.git
!pip install -q -U git+https://github.com/huggingface/accelerate.git
!pip install -q -U git+https://github.com/huggingface/trl.git

import pandas as pd
from datasets import load_dataset
from transformers import set_seed

set_seed(42)

# Load a small subset of the HelpSteer dataset (first 500 examples)
dataset = load_dataset("nvidia/HelpSteer", split="train[:500]")

# Define a function to reformat the dataset structure
def format_example(example):
    return {
        "instruction": "You are a helpful QA assistant.",  # System prompt
        "input": example["prompt"],                        # User's query
        "response": example["response"]                    # Assistant's answer
    }

# Apply the formatting function to each example in the dataset
formatted_dataset = dataset.map(format_example)

# Convert the Hugging Face dataset to a pandas DataFrame
df = pd.DataFrame(formatted_dataset)

# Display the dataframe
df[["instruction", "input", "response"]]

#### Low-Rank Adaptation (LoRA)

As LLMs are typically large, what if we do not have super powerful compute resources?

Researchers and engineers have proposed **Parameter-Efficient Fine-Tuning**.

We will explore one of its most widely used approaches: **Low-Rank Adaptation ([LoRA](https://arxiv.org/pdf/2106.09685v1/1000))**.

The intuition of LoRA is simple:

freeze the pretrained weights, but equivalently learn two **low-rank matrices $B$ and $A$**. The mathematical formulation is:

$$h = W_0x + \Delta W x = W_0x + BAx$$

<center>
    <img src="https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/assets/lora.png" width="80%">
</center>

ðŸ’¡ **Questions:**
- Q1. If pretrained $W$ is $(6,7)$, and LoRA matrices $B$, $A$ are $(6,3)$ and $(3,7)$, how many fewer parameters are trained?
  - Parameters in $W$: $6 \times 7 = 42$
  - Parameters in $B$ and $A$: $(6 \times 3) + (3 \times 7) = 18 + 21 = 39$
  - Reduction: $42 - 39 = 3$ fewer parameters


- Q2. If $W$ is $(1000,768)$ with $B$: $(1000,8)$ and $A$: $(8,768)$, what's the parameter reduction?
  - Parameters in $W$: $1000 \times 768 = 768,000$
  - Parameters in $B$ and $A$: $(1000 \times 8) + (8 \times 768) = 8,000 + 6,144 = 14,144$
  - Reduction: $768,000 - 14,144 = 753,856$ fewer parameters (â‰ˆ98% reduction)

Next, we need to load a model and plug the LoRA modules into its parameters.  
Below example is adapted from [Hands-on LLM tutorial](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter12/Chapter%2012%20-%20Fine-tuning%20Generation%20Models.ipynb).

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Specify the model checkpoint
base_model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(base_model_id)
model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    device_map="auto",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32  # Use half precision if GPU is available
)

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# Define LoRA configuration for efficient fine-tuning
peft_config = LoraConfig(
    lora_alpha=16,     # Scaling factor for LoRA layers
    lora_dropout=0.1,
    r=8,               # Rank of the low-rank matrices
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=['q_proj', 'k_proj', 'v_proj']  # Apply LoRA to attention projections
)

# Wrap the model with PEFT and inject LoRA adapters
model = get_peft_model(model, peft_config)

# Print trainable parameter names and their status
for name, param in model.named_parameters():
    print(f" {name}, Trainable: {param.requires_grad}")

# Show a summary of trainable vs total parameters
model.print_trainable_parameters()

ðŸ’¡ **Questions:**
- Q1. Can you identify what parameters are trainable?
  - `lora_A` and `lora_B` for weights `self_attn.q_proj`, `self_attn.v_proj` and `self_attn.k_proj` in every transformer layer.

- Q2. What's the ratio of trainable parameters?
  - The trainable parameter ratio is 13.91%.

from transformers import AutoTokenizer

# Load the tokenizer for the model
tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")

# Format each example into a single prompt string
def build_prompt(example):
    return {
        "text": f"{example['instruction']}\n\nUser: {example['input']}\n\nAssistant: {example['response']}"
    }

# Apply prompt formatting
train_dataset = formatted_dataset.map(build_prompt)

# Tokenize the prompt text
def tokenize(example):
    return tokenizer(
        example["text"],
        truncation=True,        # Truncate sequences longer than max_length
        padding="max_length",   # Pad shorter sequences to max_length
        max_length=512,         # Set maximum input length
    )

# Apply tokenization
train_dataset = train_dataset.map(tokenize)

from transformers import TrainingArguments
from trl import SFTTrainer

# Set up training arguments for supervised fine-tuning
training_args = TrainingArguments(
    output_dir="./tinyllama-lora-output",
    per_device_train_batch_size=8,          # Batch size per device
    gradient_accumulation_steps=4,
    optim="paged_adamw_32bit",              # Paged optimizers which take advantage of CUDAs unified memory to transfer memory from the GPU to the CPU when GPU memory is exhausted.
    learning_rate=1e-4,
    num_train_epochs=5,                     # Number of training epochs
    logging_steps=10,                       # Log metrics every 10 steps
    fp16=torch.cuda.is_available(),         # Use FP16 if GPU is available
    report_to=[],                           # Disable reporting to external loggers
)

[Paged optimizer](https://huggingface.co/docs/bitsandbytes/main/en/reference/optim/adamw#bitsandbytes.optim.PagedAdamW): take advantage of CUDAs unified memory to transfer memory from the GPU to the CPU when GPU memory is exhausted.

# Initialize SFT trainer

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    args=training_args,
    peft_config=peft_config,
)

# Train
trainer.train()

from IPython.display import display, Markdown

# Load a small validation split for quick inspection
val_dataset = load_dataset("nvidia/HelpSteer", split="validation[:20]")

# Choose a sample to test, feel free to change it between 0~19.
sample_idx = 19
test_sample = val_dataset[sample_idx]

# Print the selected sample's input and expected response
print("===== Test Sample =====")
print(f"Input question: {test_sample['prompt']}")
print(f"Ground truth: {test_sample['response']}")

# Construct the prompt for inference
instruction = "You are a helpful QA assistant."
prompt = f"{instruction}\n\nUser: {test_sample['prompt']}\n\nAssistant: "

# Tokenize the prompt and move it to the model's device
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Generate response from the model

with torch.no_grad():
    outputs = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_new_tokens=100,            # Limit the number of generated tokens
        temperature=0.3,
        top_p=0.9,
        do_sample=True,                # Enable sampling for diversity
        repetition_penalty=1.2,
        pad_token_id=tokenizer.eos_token_id  # Avoid padding warning
    )

# Decode the generated tokens into text
generated_response = tokenizer.decode(
    outputs[0][inputs.input_ids.shape[1]:],  # Remove the prompt from output
    skip_special_tokens=True
)

# Display the model's prediction
print("\n===== Model Prediction =====")
display(Markdown(generated_response))

ðŸ’¡ **Reflection:**

Evaluating LLM performance is more complex than classification tasks, where accuracy is easy to compute.

For example, the [Google Gemma](https://arxiv.org/pdf/2403.08295) model uses two main evaluation approaches:

- **Structured Automated Benchmarks:** Tasks like **multiple-choice questions**, **math problems**, or **coding with test cases** allow for clear metrics like accuracy or pass rate. Examples include [MMLU](https://huggingface.co/datasets/cais/mmlu) (common sense), [GSM8K](https://huggingface.co/datasets/openai/gsm8k) (math), and [HumanEval](https://huggingface.co/datasets/openai/openai_humaneval) (coding).

- **Human Preference Evaluation:** For open-ended tasks, human judges compare  different model outputs and pick a **winner** based on qualities like helpfulness, clarity, or creativity.

#### 2. Human Preference Optimization (RLHF & DPO)

What do we really mean when we talk about **human preference**?

Letâ€™s consider the following example (from Anthropic's [hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf/viewer/default/train?views%5B%5D=train&row=2) (Helpfulness and Harmlessness RLHF) dataset).

Which response do you think is betterâ€”and why?

<center>
    <img src="https://raw.githubusercontent.com/weijun-l/comp8420-25s1/main/assets/human_feedback.png" width="70%">
</center>

So, for an SFT model, even the same prompt can lead to very different responsesâ€”some of which may be *untruthful, toxic, or simply not helpful to the user* (as noted in the [OpenAI RLHF](https://arxiv.org/pdf/2203.02155) paper).

That raises a question: **Can we somehow use preference data pairs to guide the model toward better alignment with human intent?**

This leads to *Human Preference Optimization*â€”techniques like:
- **Reinforcement Learning from Human Feedback ([RLHF](https://arxiv.org/pdf/2203.02155), [OpenAI Intro](https://openai.com/index/instruction-following/))** (also known as PPO,  proximal policy optimization)
- **Direct Preference Optimization ([DPO](https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf))**  
which continue training the model based on human judgments.

The framework of RLHF and DPO is shown in the figure below:

Source: [Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf), [HuggingFace tutorial](https://huggingface.co/docs/trl/en/dpo_trainer)

<center>
    <img src="https://github.com/huggingface/trl/assets/49240599/9150fac6-3d88-4ca2-8ec6-2a6f3473216d" width="70%">
</center>



Pipelines:

- **RLHF**: SFT â†’ Preference Data â†’ Reward Model â†’ RL algorithm (PPO) to update SFTed model  
- **DPO**: SFT â†’ Preference Data â†’ Update SFTed model

In today's workshop, we'll explore using DPO to fine-tune a model on human preference data.

### First, let's load a human preference dataset.
### Adapted from: https://github.com/HandsOnLLM/Hands-On-Large-Language-Models

from datasets import load_dataset

# Load the UltraFeedback dataset with binary preference labels
train_dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")

# Shuffle and select a small subset for quick experimentation
sampled_dataset = train_dataset.shuffle(seed=42).select(range(100))

You can view the details of the dataset on [Huggingface](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).

## Check the structure of the dataset
sampled_dataset

Below DPO demo is adapted from both [HuggingFace tutorial](https://huggingface.co/docs/trl/en/dpo_trainer) and [Hands-on LLM tutorial](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter12/Chapter%2012%20-%20Fine-tuning%20Generation%20Models.ipynb#scrollTo=m6IfkvLkylVD).

For BitsAndBytes settings (Quantized model, used to compress the model and make training more efficient), refer to this [Hugging Face documentation](https://huggingface.co/docs/transformers/en/quantization/bitsandbytes).

from trl import DPOConfig, DPOTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# Define quantization settings for 4-bit loading
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,              # Use nested quantization for efficiency
    bnb_4bit_quant_type="nf4",                   # Use NormalFloat4 quantization
    bnb_4bit_compute_dtype="float16"             # Compute in float16 for speed
)

# Load the base model with quantization and automatic device placement
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2-0.5B-Instruct",
    quantization_config=bnb_config,
    device_map="auto"
)

# Load the corresponding tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B-Instruct")

# Prepare the quantized model for parameter-efficient fine-tuning
model = prepare_model_for_kbit_training(model)

# Define LoRA configuration to inject adapters into key transformer components
lora_config = LoraConfig(
    r=16,                   # Rank of the LoRA projection matrices
    lora_alpha=32,          # Scaling factor for the LoRA updates
    target_modules=[        # Inject LoRA into these transformer layers
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    lora_dropout=0.05,      # Dropout applied within LoRA layers
    bias="none",
    task_type="CAUSAL_LM"   # Task type: causal language modeling
)

# Apply LoRA to the base model
model = get_peft_model(model, lora_config)

# Display the number of trainable parameters
model.print_trainable_parameters()

from trl import DPOConfig

output_dir = "./results"

# Define training configuration for DPO
training_arguments = DPOConfig(
    output_dir=output_dir,
    per_device_train_batch_size=2,            # Batch size per device
    gradient_accumulation_steps=4,            # Accumulate gradients to simulate larger batch size
    optim="paged_adamw_8bit",                 # Memory-efficient optimizer
    learning_rate=5e-5,
    lr_scheduler_type="cosine",               # Use cosine learning rate decay
    max_steps=13,                             # Total number of training steps
    logging_steps=2,
    fp16=True,                                # Use half precision for faster training
    gradient_checkpointing=True,              # Reduce memory usage by re-computing activations
    warmup_ratio=0.1,
    report_to=[],
)

# Initialize the DPO trainer with model, tokenizer, config, and data
trainer = DPOTrainer(
    model=model,
    args=training_arguments,
    processing_class=tokenizer,
    train_dataset=sampled_dataset
)

# Start training with Direct Preference Optimization
trainer.train()

import torch
from IPython.display import display, Markdown

# Set model to evaluation mode
model.eval()

# Define a user query
query = 'What is the best programming language?'

# Create a chat-style prompt with system and user messages
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": f"{query}"}
]

# Format the conversation using the chat template
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Tokenize the formatted prompt
inputs = tokenizer(
    text,
    return_tensors="pt",
    padding=True,
    return_attention_mask=True
).to(model.device)

# Generate model response
with torch.no_grad():
    outputs = model.generate(
        inputs.input_ids,
        max_new_tokens=500,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

# Decode and display the generated response
response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
print("Model response: ")
display(Markdown(response))

## **This workshop aims to help you:**
- Apply **Supervised Fine-Tuning (SFT)** and **LoRA** to teach models new behaviors or domain-specific tasks efficiently.
- Understand how **human preference optimization** techniques like **RLHF** and **DPO** can align language models with human values and preferences.